{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alifzl/Zhaav-MINER-Scripts/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0266b8",
      "metadata": {
        "id": "0f0266b8"
      },
      "outputs": [],
      "source": [
        "from typing import Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ef729e",
      "metadata": {
        "id": "59ef729e"
      },
      "outputs": [],
      "source": [
        "def createSparkSession(appname:str):\n",
        "        '''\n",
        "        create spark session rdd.\n",
        "            Parameters:\n",
        "            ---------- \n",
        "                    appname (str): The appliction name  \n",
        "                    \n",
        "\n",
        "            Returns:\n",
        "                    SparkSession\n",
        "\n",
        "        '''\n",
        "        \n",
        "        import pyspark\n",
        "        from pyspark.sql import SparkSession\n",
        "        spark = SparkSession.builder.appName(appname).getOrCreate()\n",
        "        return SparkSession\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf46a57",
      "metadata": {
        "id": "fcf46a57"
      },
      "source": [
        "*Read* CSV file\n",
        "Read CSV file into DataFrame with schema and header\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b6c27a",
      "metadata": {
        "id": "24b6c27a"
      },
      "outputs": [],
      "source": [
        "def readcsvfile(header:str,inferSchema:str,appname:str,path:str,sep:str):\n",
        "        '''\n",
        "        create dataframe by reading csv file.\n",
        "            Parameters:\n",
        "            ---------- \n",
        "                    header(str):  auto detect header of the file  ,\n",
        "                  \n",
        "                    inferSchema(str):  auto detect type of columns,\n",
        "                    \n",
        "                    appname(str):  The appliction name,\n",
        "                    \n",
        "                    path(str):    file path\n",
        "                    \n",
        "                    sep(str): columns separator\n",
        "\n",
        "            Returns\n",
        "        --------------\n",
        "               dataframe( :class:`DataFrame) : return dataframe out of csv file\n",
        "\n",
        "        '''\n",
        "        spark=createSparkSession(appname)\n",
        "        dataframe=spark.read.csv(path,header=header,inferSchema=inferSchema,sep=sep) \n",
        "        return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vypOjgAWYXRr",
      "metadata": {
        "id": "vypOjgAWYXRr"
      },
      "source": [
        "*Read* multiply CSV file\n",
        "Read CSV file into DataFrame with schema and header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fD9PXwnrYWZK",
      "metadata": {
        "id": "fD9PXwnrYWZK"
      },
      "outputs": [],
      "source": [
        "def multireading(mainpath:str)->list:\n",
        "        '''\n",
        "        create dataframe by reading csv files.\n",
        "\n",
        "          Parameters:\n",
        "          ----------                   \n",
        "                    mainpath(str):  files path\n",
        "                    \n",
        "\n",
        "            Returns\n",
        "        --------------\n",
        "              `:class:DataFrame\n",
        "          '''\n",
        "        import os\n",
        "        df_list = []\n",
        "        for filename in os.listdir(mainpath):\n",
        "          if filename.endswith('.csv'):\n",
        "            filename_list = filename.split(\".\")\n",
        "            df_name=filename_list[0]\n",
        "            df=spark.read.csv(mainpath+filename,inferSchema=\"true\",header=\"true\")\n",
        "            df.name=df_name\n",
        "            df_list.append(df_name)\n",
        "        return df_list   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb69a348",
      "metadata": {
        "id": "eb69a348"
      },
      "source": [
        "read parquet files:\n",
        "read parquet filees by pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZyDk3oRusv7_",
      "metadata": {
        "id": "ZyDk3oRusv7_"
      },
      "outputs": [],
      "source": [
        "def readparquetfile(header,inferSchema,path:str):               \n",
        "    '''\n",
        "        create dataframe by reading parquet file.\n",
        "        \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                    header(str):  auto detect header of the file  ,\n",
        "                  \n",
        "                    inferSchema(str):  auto detect type of columns,\n",
        "                    \n",
        "                    appname(str):  The appliction name,\n",
        "                    \n",
        "                    path(str):    file path\n",
        "                    \n",
        "\n",
        "            Returns\n",
        "        --------------\n",
        "                :class:`DataFrame\n",
        "    '''\n",
        "\n",
        "    dataframe=spark.read.parquet(path,header=header,inferSchema=inferSchema) \n",
        "    return dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O052LTzDtRYi",
      "metadata": {
        "id": "O052LTzDtRYi"
      },
      "source": [
        "read excel files:\n",
        "read excel filees by pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eacfb3df",
      "metadata": {
        "id": "eacfb3df"
      },
      "outputs": [],
      "source": [
        "def readexcelfile(appname:str,path:str):\n",
        "        '''\n",
        "        create dataframe by reading excel file.\n",
        "        \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "                    appname(str):  The appliction name,\n",
        "                    \n",
        "                    path(str):    file path\n",
        "                    \n",
        "\n",
        "            Returns\n",
        "        --------------\n",
        "                :class:`DataFrame\n",
        "        '''        \n",
        "        import pandas as pd\n",
        "        spark=createSparkSession(appname)\n",
        "        dataframe= pd.read_excel(path)\n",
        "        dataframe_pyspark = spark.createDataFrame(dataframe)\n",
        "        return dataframe_pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "friAkvoTSkci",
      "metadata": {
        "id": "friAkvoTSkci"
      },
      "source": [
        "### writing file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x_pFj7INSjQH",
      "metadata": {
        "id": "x_pFj7INSjQH"
      },
      "outputs": [],
      "source": [
        "def writhingfile(dataframe,name:str,mode:str):\n",
        "    '''\n",
        "  write csv file.\n",
        "  \n",
        "  Parameters:\n",
        "  -------------- \n",
        "          dataframe(:class:`DataFrame) : select dataframe that want to write\n",
        "          name(str) : name of the file \n",
        "          mode(str) : append or write\n",
        "    '''\n",
        "  \n",
        "  \n",
        "    from py4j.java_gateway import java_import\n",
        "    dataframe.write.mode(mode).csv(name+'.csv')\n",
        "    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
        "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
        "    file = fs.globStatus(spark._jvm.Path(name+\".csv/part*\"))[0].getPath().getName()\n",
        "    fs.rename(spark._jvm.Path(name+'.csv/' + file), spark._jvm.Path(name+'2.csv'))\n",
        "    fs.delete(spark._jvm.Path(name+'.csv'), True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7707c29e",
      "metadata": {
        "id": "7707c29e"
      },
      "source": [
        "### common command\n",
        "includes show, info,types,statistic describe,name of the columns and select."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mpTKdjmVq6j2",
      "metadata": {
        "id": "mpTKdjmVq6j2"
      },
      "outputs": [],
      "source": [
        "def convert_npandas(dataframe,number:int=1):\n",
        "    \n",
        "        '''\n",
        "        create pandas dataframe from spark data frame .\n",
        "        \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "                dataframe(:class:`DataFrame) : select dataframe that  convert to pandas dataframe\n",
        "                    \n",
        "                number(int):   mandatory , number of rows\n",
        "                    \n",
        "\n",
        "            Returns\n",
        "        --------------\n",
        "                :class:`DataFrame\n",
        "        '''\n",
        "        dataframe_show=dataframe.limit(number).toPandas()\n",
        "        return dataframe_show\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568c6eb4",
      "metadata": {
        "id": "568c6eb4"
      },
      "outputs": [],
      "source": [
        "def show(dataframe,number:int=None,truncate:bool=None):\n",
        "        '''\n",
        "        show the dataframe .\n",
        "        \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "                    \n",
        "            number(int) : optional, number of rows to show.\n",
        "            \n",
        "            truncate : bool, optional\n",
        "            If set to ``True``, truncate strings longer than 20 chars by default.\n",
        "            If set to a number greater than one, truncates long strings to length ``truncate``\n",
        "            and align cells right.\n",
        "            \n",
        "            Returns\n",
        "        --------------\n",
        "               \"\"\"Prints the first ``n`` rows to the console.\n",
        "\n",
        "    \n",
        "\n",
        "        \n",
        "\n",
        "                '''\n",
        "        dataframe.show(number,truncate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49741662",
      "metadata": {
        "id": "49741662"
      },
      "outputs": [],
      "source": [
        "def dataframetype(dataframe):\n",
        "    \"\"\"Returns all column names and their data types as a list\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "                    \n",
        "        Returns\n",
        "        --------------\n",
        "               Prints types of the rows \"\"\"\n",
        "    dataframe.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7906b807",
      "metadata": {
        "id": "7906b807"
      },
      "outputs": [],
      "source": [
        "def statisticdescribe(dataframe):\n",
        "        \"\"\"Returns statistics summary of dataframe\n",
        "        \n",
        "        Parameters:\n",
        "        --------------\n",
        "        dataframe(:class:`DataFrame) : select dataframe\n",
        "\n",
        "        Returns\n",
        "        --------------\n",
        "              :class:`DataFrame\"\"\"\n",
        "        dataframe_new=dataframe.describe()\n",
        "        return dataframe_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac17b61",
      "metadata": {
        "id": "fac17b61"
      },
      "outputs": [],
      "source": [
        "def coloumes(dataframe):\n",
        "    \"\"\"Returns all column names  as a list\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "                    \n",
        "        Returns\n",
        "        --------------\n",
        "               Prints all column names \"\"\"\n",
        "    dataframe.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ab898b",
      "metadata": {
        "id": "93ab898b"
      },
      "outputs": [],
      "source": [
        "def select(dataframe,coloumes:Union[list, str]):\n",
        "    \"\"\"select specfic columns by name and return new dataframe\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "            \n",
        "            coloumes(str/list): list of string coloumes names         \n",
        "\n",
        "        Returns\n",
        "        --------------\n",
        "              dataframe_new(:class:`DataFrame ): return new dataframe\n",
        "              \n",
        "              \n",
        "        Example\n",
        "        -------------- \n",
        "            1- common select \n",
        "                \n",
        "                dataframe_new=dataframe.select(\"name\")\n",
        "\n",
        "\n",
        "            \n",
        "            \n",
        "            2- also can slicing the dataframe with start and end\n",
        "            \n",
        "                dataframe_new=dataframe.select(dataframe.columns[start=10:end=20])\"\"\"\n",
        "    dataframe_new=dataframe.select(coloumes)\n",
        "    return dataframe_new\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lDmwiLqUsP1i",
      "metadata": {
        "id": "lDmwiLqUsP1i"
      },
      "outputs": [],
      "source": [
        "def collect(dataframe)->list:\n",
        "  \"\"\" return all of dataframe as list\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "            \n",
        "                   \n",
        "\n",
        "        Returns\n",
        "        --------------\n",
        "              ListData(list): list of all dataframe \"\"\"\n",
        "\n",
        "  ListData=dataframe.collect()\n",
        "  return ListData"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260d655b",
      "metadata": {
        "id": "260d655b"
      },
      "source": [
        "## Common filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a_HHqazHbGuI",
      "metadata": {
        "id": "a_HHqazHbGuI"
      },
      "source": [
        "### starts with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ig88wu_IbGYs",
      "metadata": {
        "id": "Ig88wu_IbGYs"
      },
      "outputs": [],
      "source": [
        "def startswith(dataframe,columns:str,value:str):\n",
        "    \"\"\" return all row of dataframe as a new dataframe that starts with the value  \n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "\n",
        "            columns(str) : columns of dataframe\n",
        "            \n",
        "                   \n",
        "\n",
        "        Returns\n",
        "        --------------\n",
        "              dataframe_new(:class:`DataFrame): dataframe of the value\"\"\"\n",
        "    dataframe_new=dataframe.where(dataframe[columns].startswith(value))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eXL3Hr2Xbzao",
      "metadata": {
        "id": "eXL3Hr2Xbzao"
      },
      "source": [
        "### endswith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h6KMOT__boSE",
      "metadata": {
        "id": "h6KMOT__boSE"
      },
      "outputs": [],
      "source": [
        "def endswith(dataframe,columns:str,value:str):\n",
        "    \"\"\" return all row of dataframe as a new dataframe that ends with the value \n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "\n",
        "            columns(str) : columns of dataframe\n",
        "            \n",
        "                   \n",
        "\n",
        "        Returns\n",
        "        --------------\n",
        "              dataframe_new(:class:`DataFrame): dataframe of the value\"\"\"\n",
        "    dataframe_new=dataframe.where(dataframe[columns].endswith(value))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd6a253",
      "metadata": {
        "id": "ddd6a253"
      },
      "outputs": [],
      "source": [
        "def filt(dataframe,condition:str):\n",
        "    \"\"\" return all row of dataframe as dataframe that filter by condition \n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "\n",
        "            columns(str) : columns of dataframe\n",
        "            \n",
        "                   \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "              dataframe_new(:class:`DataFrame): dataframe of the value\n",
        "              \n",
        "        Example:\n",
        "        --------------\n",
        "        dataframe_new = filt (df,df[\"Ward\"]==\"value\") \n",
        "\n",
        "              \"\"\"\n",
        "        \n",
        "        \n",
        "    dataframe_new=dataframe.filter(condition)\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40fa806d",
      "metadata": {
        "id": "40fa806d"
      },
      "source": [
        "sort data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def groupbydesc(dataframe,columnname:str):\n",
        "  \"\"\" return  a sorted dataframe by  columnname\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "\n",
        "            columnname(str):select the column\n",
        "        Returns:\n",
        "        --------------\n",
        "            df_sorted (:class:`DataFrame) : new sorted dataframe \n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "  df_sorted = dataframe.groupby(columnname).count().orderBy(col(\"count\").desc())\n",
        "  return df_sorted"
      ],
      "metadata": {
        "id": "OOIOpUmyTyMb"
      },
      "id": "OOIOpUmyTyMb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ef1221",
      "metadata": {
        "id": "78ef1221"
      },
      "outputs": [],
      "source": [
        "def sortascending(dataframe,coloumes:str,ascending:bool):\n",
        "    \"\"\" return all sorted  dataframe with  ascending or descending method\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "\n",
        "            columns(str) : columns of dataframe\n",
        "            \n",
        "            ascending(bool): if ascending equal to true it's sorted by ascending if it's equal to False sorted descending\n",
        "            \n",
        "                   \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_asc (:class:`DataFrame) : dataframe of sort value\"\"\"\n",
        "\n",
        "    dataframe_asc = dataframe.orderBy(coloumes,ascending=ascending) \n",
        "    return dataframe_asc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VO64F2m8XyGd",
      "metadata": {
        "id": "VO64F2m8XyGd"
      },
      "outputs": [],
      "source": [
        "def isin (dataframe,columns:str,value:str):\n",
        "   \"\"\" return  a  dataframe  where the columns contain Specific  value\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe\n",
        "\n",
        "            columns(str) : columns of dataframe\n",
        "            \n",
        "            value(str): value in columns\n",
        "                   \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : datafram contain Specific  value\"\"\"\n",
        "   dataframe_new = dataframe[dataframe[columns].isin(value)]\n",
        "   return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809e553b",
      "metadata": {
        "id": "809e553b"
      },
      "source": [
        "### Join two dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba261e20",
      "metadata": {
        "id": "ba261e20"
      },
      "outputs": [],
      "source": [
        "def join(dataframe1,dataframe2,list_coloumes:str,how:str):\n",
        "    \"\"\" return  a  dataframe  combinong two dataframes\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe1(:class:`DataFrame) : select dataframe  number 1\n",
        "\n",
        "            dataframe2(:class:`DataFrame) : select dataframe  number 2\n",
        "\n",
        "            list_coloumes(str) : columns of dataframe\n",
        "            \n",
        "            how(str):   inner', 'outer', 'full', 'fullouter', 'full_outer', \n",
        "                        'leftouter', 'left', 'left_outer', 'rightouter', 'right', \n",
        "                        'right_outer', 'leftsemi', 'left_semi', 'semi', 'leftanti', \n",
        "                        'left_anti', 'anti', 'cross' joins\n",
        "                   \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : combinong dataframe \"\"\"\n",
        "    dataframe_join=dataframe1.join(dataframe2,list_coloumes,how)\n",
        "    return  dataframe_join"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "387a7a02",
      "metadata": {
        "id": "387a7a02"
      },
      "source": [
        "make new coloumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688d42e7",
      "metadata": {
        "id": "688d42e7"
      },
      "outputs": [],
      "source": [
        "def makecoloumes(dataframe,name:str,opration):\n",
        "    \"\"\" return  a new  dataframe  from old dataframe by using opration\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            name(str):  name of new coloumes\n",
        "            opration : opertion on some coloumes  \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe \n",
        "            \n",
        "        Example:\n",
        "        --------------\n",
        "        Create a new column named c1 from twice the value of the ward column . \n",
        "            dataframe_new=dataframe.withColumn(\"c1\",2*dataframe['ward'])    \n",
        "            \"\"\"\n",
        "    dataframe_new=dataframe.withColumn(name,opration)\n",
        "    \n",
        "    return dataframe_new\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bec69d",
      "metadata": {
        "id": "66bec69d"
      },
      "source": [
        "change the type of one coloume\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66013cb8",
      "metadata": {
        "id": "66013cb8"
      },
      "outputs": [],
      "source": [
        "def changetype(dataframe,name_coloume:str,datatype:str):\n",
        "    \"\"\" return  a new  dataframe  from old dataframe with changing data type\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            name_coloume(str):  name of new coloumes\n",
        "            datatype : int,float,str \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe \n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "    from pyspark.sql.functions import col\n",
        "    dataframe_new=dataframe.withColumn(name_coloume,col = col(name_coloume).cast(datatype))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43687b62",
      "metadata": {
        "id": "43687b62"
      },
      "outputs": [],
      "source": [
        "def changedate(dataframe,format:str,datecoloume):\n",
        "    \"\"\" return  a new  dataframe  from old dataframe with changing data type (time )\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            datecoloume(str):  name of time coloumes\n",
        "            format(str) : the main format yyyy-MM-dd HH:mm:ss. SSSS  \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe \n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "    from pyspark.sql.functions import to_date\n",
        "    dataframe_new = dataframe.select(to_date(datecoloume,format))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8WbkVs-Ee3K3",
      "metadata": {
        "id": "8WbkVs-Ee3K3"
      },
      "source": [
        "### concat two columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7CuCOTKTe2VF",
      "metadata": {
        "id": "7CuCOTKTe2VF"
      },
      "outputs": [],
      "source": [
        "def concatcolumn(dataframe,column1:str,column2:str,aliasname:str,sep:str):\n",
        "    \"\"\" return  a  dataframe  combinong two dataframes\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            column1(str):  column name 1\n",
        "            column2(str):  column name 2 \n",
        "            aliasname(str) : name of new column \n",
        "            sep(str) : seperator \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe \n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "    from pyspark.sql.functions import concat_ws\n",
        "    dataframe_new =dataframe.select(concat_ws(sep,dataframe[column1],dataframe[column2]).alias(aliasname))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GA1Lv_-3j4NH",
      "metadata": {
        "id": "GA1Lv_-3j4NH"
      },
      "source": [
        "### new column by splite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OGacI7TVj3xE",
      "metadata": {
        "id": "OGacI7TVj3xE"
      },
      "outputs": [],
      "source": [
        "def spliting(dataframe,column:str,sep:str,aliasname:str):\n",
        "  \"\"\" create  a new dataframe from spliting the old dataframe\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            column(str):  column name \n",
        "            \n",
        "            aliasname(str) : name of new column \n",
        "            sep(str) : seperator \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe \n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "  from pyspark.sql.functions import split\n",
        "  dataframe_new=dataframe.withColumn(aliasname,split(dataframe[column],sep))\n",
        "  return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79aec8d7"
      },
      "source": [
        "### Construct a new dynamic column From two columns \n"
      ],
      "id": "79aec8d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f983a3d"
      },
      "outputs": [],
      "source": [
        "def newcolumn(dataframe,newcolumnname:str,column1:str,column2:str):\n",
        "    \"\"\" Construct a new dynamic column From two columns \n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            newcolumnname(str):  column name \n",
        "            \n",
        "            column1(str) : name of  column 1\n",
        "            column2(str) : name of  column 2 \n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe \n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumnname, F.when((dataframe[column1].isNotNull() & dataframe[column2].isNotNull())\n",
        "                                     , F.concat(dataframe[column1], dataframe[column2])).otherwise(F.lit(None)))\n",
        "    return dataframe_new"
      ],
      "id": "9f983a3d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqXvqkFcBL4R"
      },
      "source": [
        "### Remove columns\n"
      ],
      "id": "FqXvqkFcBL4R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e28YPFNLBOHD"
      },
      "outputs": [],
      "source": [
        "def removecolumns(dataframe,columnsname:str):\n",
        "  \"\"\" remove specific  column in dataframe \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove a column\n",
        "            \n",
        "  \"\"\"\n",
        "\n",
        "  dataframe_new = dataframe.drop(columnsname)\n",
        "  return dataframe_new"
      ],
      "id": "e28YPFNLBOHD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80c1fb71"
      },
      "source": [
        "### remove other columns of dataframe if specific column doesn't have the character"
      ],
      "id": "80c1fb71"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fed31307"
      },
      "outputs": [],
      "source": [
        "def removecontain(dataframe,column:str,character:str):\n",
        "      \n",
        "\"\"\" remove other columns of dataframe if specific column doesn't have the character\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove  columns\n",
        "            \n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    dataframe = dataframe.filter(dataframe[column].contains(character))\n",
        "    return dataframe"
      ],
      "id": "fed31307"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b97d9ab"
      },
      "source": [
        "### remove other columns of dataframe  if the column doesn't start with a specific character"
      ],
      "id": "5b97d9ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f119df7"
      },
      "outputs": [],
      "source": [
        "def removestart(dataframe,column,character:str):\n",
        "          \n",
        "    \"\"\" remove other columns of dataframe  if the column doesn't start with a specific character\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove  columns\n",
        "            \n",
        "    \"\"\"\n",
        "\n",
        "    dataframe = dataframe.filter(dataframe[column].startswith(character))\n",
        "    return dataframe\n",
        "    "
      ],
      "id": "3f119df7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4e1c82"
      },
      "source": [
        "### remove other columns of dataframe  if the column doesn't end with a specific character\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "0e4e1c82"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14f94fa3"
      },
      "outputs": [],
      "source": [
        "def removeend(dataframe,column,character:str):\n",
        "              \n",
        "    \"\"\" remove whole dataframe if it doesn't end with a specific character\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove  columns\n",
        "            \n",
        "    \"\"\"\n",
        "    dataframe = dataframe.filter(dataframe[column].endswith(character))\n",
        "    return dataframe"
      ],
      "id": "14f94fa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3864520"
      },
      "source": [
        "### remove whole dataframe if it has null values"
      ],
      "id": "e3864520"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5380a4cc"
      },
      "outputs": [],
      "source": [
        "def removenul(dataframe,column):\n",
        "   \n",
        "    \"\"\" remove whole dataframe if it has null values\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove  columns\n",
        "            \n",
        "    \"\"\"\n",
        "    dataframe = dataframe.filter(dataframe[column].isnull())\n",
        "    return dataframe\n",
        "\n"
      ],
      "id": "5380a4cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeda3c20"
      },
      "source": [
        "### remove whole dataframe if it has'nt null value"
      ],
      "id": "aeda3c20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d47adeb2"
      },
      "outputs": [],
      "source": [
        "def removenotnul(dataframe,column):\n",
        "    \"\"\" ### remove whole dataframe if it has'nt null value\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove a column\n",
        "            \n",
        "    \"\"\"\n",
        "    dataframe = dataframe.filter(dataframe[column].isnotnull())\n",
        "    return dataframe"
      ],
      "id": "d47adeb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "417b2e3a"
      },
      "source": [
        "### remove dataframe if column name doesn't start with specific character with like"
      ],
      "id": "417b2e3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b13d0284"
      },
      "outputs": [],
      "source": [
        "def removelike(dataframe,column,character:str):\n",
        "    \"\"\" remove whole dataframe if it doesn't contain a specific character\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove  columns\n",
        "            \n",
        "    \"\"\"\n",
        "\n",
        "    dataframe = dataframe.filter(dataframe[column].like(character))\n",
        "    return dataframe"
      ],
      "id": "b13d0284"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a64ea45"
      },
      "outputs": [],
      "source": [
        "def removeregex(dataframe,column,character:str):\n",
        "    \n",
        "    \"\"\" remove whole dataframe if it doesn't contain a specific character with regex\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove  columns\n",
        "            \n",
        "    \"\"\"\n",
        "    \n",
        "    dataframe = dataframe.filter(dataframe[column].rlike(character))\n",
        "    return dataframe"
      ],
      "id": "5a64ea45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WbW47Bz2LFE"
      },
      "source": [
        "### writing Query"
      ],
      "id": "6WbW47Bz2LFE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c636e25e"
      },
      "outputs": [],
      "source": [
        "def createQuery(dataframe,name:str,Query:str):\n",
        "   \"\"\" Construct a new datafram(table) with sql Query\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            name(str):   table name\n",
        "            Query(str): sql Query\n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            sparksql (:class:`DataFrame) : new dataframe\n",
        "\n",
        "\n",
        "\n",
        "          Examples\n",
        "        --------\n",
        "        >>> df.createOrReplaceTempView(\"people\")\n",
        "        >>> df2 = df.filter(df.age > 3)\n",
        "        >>> df2.createOrReplaceTempView(\"people\")\n",
        "        >>> df3 = spark.sql(\"select * from people\")\n",
        "        >>> sorted(df3.collect()) == sorted(df2.collect())\n",
        "        True\n",
        "        >>> spark.catalog.dropTempView(\"people\")\n",
        "        \"\"\"\n",
        "   dataframe.createOrReplaceTempView(name)\n",
        "   sparksql=spark.sql(Query)\n",
        "   return sparksql"
      ],
      "id": "c636e25e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz19pLFHMZuH"
      },
      "source": [
        "### create a object by Query "
      ],
      "id": "dz19pLFHMZuH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PISgwcxAMZa0"
      },
      "outputs": [],
      "source": [
        "def Querytransfor(dataframe,Query:str):\n",
        "    \"\"\" Construct a new datafram with sql Query\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            Query(str): sql Query\n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    from pyspark.ml.feature import SQLTransformer\n",
        "    sqltrans=SQLTransformer(statement=Query+\"from __THIS__\")\n",
        "    dataframe_new = sqltrans.transform(dataframe)\n",
        "    return dataframe_new"
      ],
      "id": "PISgwcxAMZa0"
    },
    {
      "cell_type": "markdown",
      "id": "hy5DL6t9CYHe",
      "metadata": {
        "id": "hy5DL6t9CYHe"
      },
      "source": [
        "### null counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vrfzBCq5CdqJ",
      "metadata": {
        "id": "vrfzBCq5CdqJ"
      },
      "outputs": [],
      "source": [
        "def null_value_calc(dataframe)->list:\n",
        "  \"\"\" return  a  list of null value\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "\n",
        "        Returns:\n",
        "        --------------\n",
        "            (null_columns_counts) (list) : list of null value\n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "  from pyspark.sql.functions import col\n",
        "  null_columns_counts=[]\n",
        "  numrows=dataframe.count()\n",
        "  for k in dataframe.columns:\n",
        "    nullrows=dataframe.where(col(k).isNull()).count()\n",
        "    if (nullrows>0):\n",
        "      temp = k,nullrows,(nullrows/numrows)*100\n",
        "      null_columns_counts.append(temp)\n",
        "  return (null_columns_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b968b6",
      "metadata": {
        "id": "40b968b6"
      },
      "source": [
        "#### Replace all nulls with a specific value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf3987f",
      "metadata": {
        "id": "4bf3987f"
      },
      "outputs": [],
      "source": [
        "def replacenull(dataframe,columns:str,newvalue:str):\n",
        "    \"\"\" replace all missing data with optional value \n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newvalue(str) : optional value\n",
        "\n",
        "            columns(str) : name of  column\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with missing replace \n",
        "            \n",
        "\n",
        "        \"\"\"\n",
        "    dataframe = dataframe.fillna({columns: newvalue})\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9028049c",
      "metadata": {
        "id": "9028049c"
      },
      "source": [
        "#### fill all nulls with a specific value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WsQjbD47R3Jf",
      "metadata": {
        "id": "WsQjbD47R3Jf"
      },
      "outputs": [],
      "source": [
        "def fillnull(dataframe,newvalue:Union[str,int],subset:list):\n",
        "  \"\"\" replace all missing data with optional value for entire dataframe\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newvalue(str,int) : optional value\n",
        "\n",
        "            columns(str) : name of  column\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with missing replace \n",
        "            \n",
        "\n",
        "        \"\"\"\n",
        "  dataframe_new=dataframe.na.fill(newvalue,subset)\n",
        "  return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZhBbcs8FOLWs",
      "metadata": {
        "id": "ZhBbcs8FOLWs"
      },
      "source": [
        "#### drop null values\n",
        "How and Thresh and Subset is optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7GtL4_5AORMT",
      "metadata": {
        "id": "7GtL4_5AORMT"
      },
      "outputs": [],
      "source": [
        "def dropnnull(dataframe,thresh:int=None,how:str=\"any\",subset:list=None):\n",
        "  \"\"\" drop all missing data column in dataframe\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            thresh(int,optional) : optional value\n",
        "\n",
        "            how(str,optional) : include (any, all)\n",
        "            \n",
        "            subset(list,optional): list of the column\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with drop column \n",
        "            \n",
        "  \"\"\"\n",
        "  dataframe_new=dataframe.na.drop(how,thresh=thresh,subset=subset)\n",
        "  return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baa04470",
      "metadata": {
        "id": "baa04470"
      },
      "source": [
        "##### drop null by filter method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TKxZWI1cPPIf",
      "metadata": {
        "id": "TKxZWI1cPPIf"
      },
      "outputs": [],
      "source": [
        "def dropnnullbyfilter(dataframe,column:str):\n",
        "    \"\"\" drop all missing data column in dataframe using filter method\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "\n",
        "            \n",
        "            column(str):  column name \n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with drop column \n",
        "            \n",
        "  \"\"\"\n",
        "    dataframe_new=dataframe.filter(dataframe[column].isNotNull())\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ljIOkYcl7AkD",
      "metadata": {
        "id": "ljIOkYcl7AkD"
      },
      "source": [
        "#### Drop duplicate rows in a dataframe (distinct) \n",
        "columnsname is optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A9Knn4oT7AGd",
      "metadata": {
        "id": "A9Knn4oT7AGd"
      },
      "outputs": [],
      "source": [
        "def dropDuplicates(dataframe,columnsname:list):\n",
        "  \"\"\" drop all duplicate  column in dataframe \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "\n",
        "            \n",
        "            column(list,otional):  column name \n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with drop the duplicate data \n",
        "            \n",
        "  \"\"\"\n",
        "  dataframe_new = dataframe.drop_duplicates(subset=columnsname)\n",
        "  return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ztKCJhO_cqN",
      "metadata": {
        "id": "3ztKCJhO_cqN"
      },
      "source": [
        "#### Replace specific value with null \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1i8tOxsX7dE-",
      "metadata": {
        "id": "1i8tOxsX7dE-"
      },
      "outputs": [],
      "source": [
        "def replacewithnull(dataframe,specificvalue:Union[int,str],columnsname:str):\n",
        "    \n",
        "  \"\"\" replace specific value  column in dataframe with null value\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            specificvalue(int,str) : specific value \n",
        "            \n",
        "            column(list,otional):  column name \n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with specific value\n",
        "            \n",
        "  \"\"\"\n",
        "  dataframe_new = dataframe.replace({specificvalue: None}, subset=[columnsname])\n",
        "  return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf769bd"
      },
      "source": [
        "## impute null data"
      ],
      "id": "2bf769bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ac88e7"
      },
      "source": [
        "### impute null data by avarage strategy"
      ],
      "id": "57ac88e7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b616f0ab"
      },
      "outputs": [],
      "source": [
        "def imputnullavg(dataframe,inputCols:list,outputCols:list):   \n",
        "    \"\"\" return a dataframe  with imputing null value with average \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(list) : name of input columns \n",
        "\n",
        "            outputCols(list) : name of output columns \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe   with imputing null value with average \n",
        "            \n",
        "            \"\"\"\n",
        "    from pyspark.ml.feature import Imputer\n",
        "    a= Imputer(inputCols=inputCols,outputCols=outputCols)\n",
        "    a.setStrategy(\"mean\")\n",
        "    dataframe = a.fit(dataframe).transform(dataframe)\n",
        "    return dataframe"
      ],
      "id": "b616f0ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36f7b1bd"
      },
      "source": [
        "### impute null data by median strategy\n"
      ],
      "id": "36f7b1bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e353cbb7"
      },
      "outputs": [],
      "source": [
        "def imputnullmedian(dataframe,inputCols:list,outputCols:list):   \n",
        "    \"\"\" return a dataframe  with imputing null value with median \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            outputCols(str) : name of output columns \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe   with imputing null value with median \n",
        "            \n",
        "            \"\"\"\n",
        "    from pyspark.ml.feature import Imputer\n",
        "    a= Imputer(inputCols=inputCols,outputCols=outputCols)\n",
        "    a.setStrategy(\"median\")\n",
        "    dataframe=a.fit(dataframe).transform(dataframe)\n",
        "    return dataframe"
      ],
      "id": "e353cbb7"
    },
    {
      "cell_type": "markdown",
      "id": "e716314a",
      "metadata": {
        "id": "e716314a"
      },
      "source": [
        "### find basic statistic parameter to replace with null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ca3b33",
      "metadata": {
        "id": "f9ca3b33"
      },
      "outputs": [],
      "source": [
        "def findstatistict(dataframe,column:str)->dict:\n",
        "    \"\"\" return basic statistic parameter \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            avg(int): average of the column\n",
        "\n",
        "            stddev(int): standard deviation of the column\n",
        "\n",
        "            min(int): minimum of the column\n",
        "\n",
        "            max(int): maximum of the column\n",
        "\n",
        "            q1(int): quartile first of the column\n",
        "\n",
        "            q2(int): quartile second of the column\n",
        "\n",
        "            q3(int): quartile third of the column\n",
        "            \n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    avg=eval(dataframe.summary().select(\"summary\",column).collect()[1][1])\n",
        "    stddev=eval(dataframe.summary().select(\"summary\",column).collect()[2][1])\n",
        "    min=eval(dataframe.summary().select(\"summary\",column).collect()[3][1])\n",
        "    max=eval(dataframe.summary().select(\"summary\",column).collect()[7][1])\n",
        "    q1=eval(dataframe.summary().select(\"summary\",column).collect()[4][1])\n",
        "    q2=eval(dataframe.summary().select(\"summary\",column).collect()[5][1])\n",
        "    q3=eval(dataframe.summary().select(\"summary\",column).collect()[6][1])\n",
        "\n",
        "    base={\"avg\":avg,\"stddev\":stddev,\"min\":min,\"max\":max,\"quartile1\":q1,\"quartile2\":q2,\"quartile3\":q3}\n",
        "    return base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6fb7408",
      "metadata": {
        "id": "f6fb7408"
      },
      "source": [
        "## String Operations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wordsonly(dataframe,columnname:str):\n",
        "    \"\"\" return dataframe with only words and space between.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            columnname(str):  column name \n",
        "\n",
        "  \n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with trim a column\n",
        "            \n",
        "    \"\"\" \n",
        "\n",
        "    dataframe_new = dataframe.withColumn(columnname,regexp_replace(col(columnname), '[^A-Za-z]+', ' '))\n",
        "    return dataframe_new"
      ],
      "metadata": {
        "id": "GfYcKC16PHmY"
      },
      "id": "GfYcKC16PHmY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d935636",
      "metadata": {
        "id": "4d935636"
      },
      "source": [
        "### remove dataframe if column  doesn't have a specefic list name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e6dca7",
      "metadata": {
        "id": "96e6dca7"
      },
      "outputs": [],
      "source": [
        "def removelist(dataframe,column,character:Union[tuple,str]):\n",
        "    \"\"\" remove whole dataframe if it doesn't contain a specific character with regex\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            character(tuple,str): specific character name\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with remove  columns\n",
        "            \n",
        "    \"\"\"\n",
        "    dataframe = dataframe.filter(dataframe[column].isin(character))\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e162303b",
      "metadata": {
        "id": "e162303b"
      },
      "source": [
        "### String Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3227743d",
      "metadata": {
        "id": "3227743d"
      },
      "source": [
        "#### substring the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31b75076",
      "metadata": {
        "id": "31b75076"
      },
      "outputs": [],
      "source": [
        "def substrcolumn(dataframe,column:str,newcolumn:str,start:int,lenght:int):\n",
        "    \"\"\" sub string a column and make new column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            newcolumn(str): new column name \n",
        "\n",
        "            start (int): starting ponit\n",
        "            \n",
        "            lenght(int) : lenght of column \n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with substring a column\n",
        "            \n",
        "    \"\"\"\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, dataframe[column].substr(start, lenght))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b0a41f",
      "metadata": {
        "id": "e8b0a41f"
      },
      "source": [
        "#### Trim the spaces from both ends for the specified string column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a92ac98",
      "metadata": {
        "id": "9a92ac98"
      },
      "outputs": [],
      "source": [
        "def trimstrcolumn(dataframe,column,newcolumn:str):\n",
        "    \"\"\" Trim the spaces from both ends for the specified string column.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            newcolumn(str): new column name \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with trim a column\n",
        "            \n",
        "    \"\"\"\n",
        "\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe = dataframe.withColumn(newcolumn, F.trim(column))\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4660e961",
      "metadata": {
        "id": "4660e961"
      },
      "source": [
        "### left trim and right trim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b156900",
      "metadata": {
        "id": "5b156900"
      },
      "outputs": [],
      "source": [
        "def ltrim(dataframe,newcolumn:str,column:str):\n",
        "    \"\"\" left Trim the spaces from string column.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            newcolumn(str): new column name \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with trim a column\n",
        "            \n",
        "    \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.ltrim(dataframe[column]))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e39d49e",
      "metadata": {
        "id": "7e39d49e"
      },
      "outputs": [],
      "source": [
        "def rtrim(dataframe,newcolumn:str,column:str):\n",
        "    \"\"\" right Trim the spaces from string column.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            newcolumn(str): new column name \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with trim a column\n",
        "    \"\"\"        \n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.rtrim(dataframe[column]))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734b0e14",
      "metadata": {
        "id": "734b0e14"
      },
      "source": [
        "#### Left-pad the string column to width len with pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8911ac55",
      "metadata": {
        "id": "8911ac55"
      },
      "outputs": [],
      "source": [
        "def leftpad(dataframe,column,newcolumn:str,character:str,digitnumber:int):\n",
        "    \"\"\" Left-pad the string column to width len with pad.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            column(str):  column name \n",
        "\n",
        "            newcolumn(str): new column name \n",
        "\n",
        "            character(str): a character to add a column\n",
        "\n",
        "            digitnumber(int): number of character\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with trim a column\n",
        "            \n",
        "    \"\"\"\n",
        "\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.lpad(dataframe[column], digitnumber, character))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c09229",
      "metadata": {
        "id": "f4c09229"
      },
      "source": [
        "### Concatenate with character between"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7358888d",
      "metadata": {
        "id": "7358888d"
      },
      "outputs": [],
      "source": [
        "def concattwo(dataframe,newcolumn:str,column1:str,column2:str,chrbetween:str):\n",
        "    \"\"\" Concatenate with character between.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column1(str):  first column name \n",
        "            \n",
        "            column2(str):  secend column name\n",
        "\n",
        "            chrbetween(str): a character to add between a column\n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with  with a character between.\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.concat(dataframe[column1], F.lit(chrbetween), dataframe[column2]))\n",
        "    return dataframe_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00a2296",
      "metadata": {
        "id": "d00a2296"
      },
      "source": [
        "### Concatenate with Separator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602135bc",
      "metadata": {
        "id": "602135bc"
      },
      "outputs": [],
      "source": [
        "def concatwithesep(dataframe,newcolumn:str,*cols:str,chrbetween:str):\n",
        "        from pyspark.sql import functions as F\n",
        "        \"\"\"\"Returns a new :class:`DataFrame` by adding a column or replacing the\n",
        "        existing column that has the same name.\n",
        "\n",
        "        The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
        "        a column from some other :class:`DataFrame` will raise an error.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        colName : str\n",
        "            string, name of the new column.\n",
        "        col : :class:`Column`\n",
        "            a :class:`Column` expression for the new column.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        This method introduces a projection internally. Therefore, calling it multiple\n",
        "        times, for instance, via loops in order to add multiple columns can generate big\n",
        "        plans which can cause performance issues and even `StackOverflowException`.\n",
        "        To avoid this, use :func:`select` with the multiple columns at once.\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with trim a column\n",
        "            \"\"\"\n",
        "       \n",
        "        dataframe = dataframe.withColumn(newcolumn, F.concat_ws(chrbetween, *cols))\n",
        "        return dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4673af68",
      "metadata": {
        "id": "4673af68"
      },
      "source": [
        "## Number Operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cadb8e1",
      "metadata": {
        "id": "7cadb8e1"
      },
      "source": [
        "### Round the number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afee0d6",
      "metadata": {
        "id": "4afee0d6"
      },
      "outputs": [],
      "source": [
        "def roundnumber(dataframe,newcolumn:str,column:str,scale:int):\n",
        "    \"\"\" return round of integer columns\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column(str):   column name\n",
        "\n",
        "            scale(int): a scale of rounding\n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with  with a round of a columns\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.round(dataframe[column], scale))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "271cba96",
      "metadata": {
        "id": "271cba96"
      },
      "source": [
        "### Floor of the number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730dbfe9",
      "metadata": {
        "id": "730dbfe9"
      },
      "outputs": [],
      "source": [
        "def floarnumber(dataframe,newcolumn:str,column:str):\n",
        "    \"\"\" return floor of integer columns\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column(str):   column name\n",
        "\n",
        "            scale(int): a scale of rounding\n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with  with a round of a columns\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.floor(dataframe[column]))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77b923a",
      "metadata": {
        "id": "b77b923a"
      },
      "source": [
        "### Ceiling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df803a3",
      "metadata": {
        "id": "7df803a3"
      },
      "outputs": [],
      "source": [
        "def Ceilingnumber(dataframe,newcolumn:str,column:str):\n",
        "    \"\"\" return Ceilin of integer columns\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column(str):   column name\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with Ceilin a columns\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.ceil(dataframe[column]))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f2c185",
      "metadata": {
        "id": "61f2c185"
      },
      "source": [
        "### Absolute Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7548b0",
      "metadata": {
        "id": "6f7548b0"
      },
      "outputs": [],
      "source": [
        "def Absolutenumber(dataframe,newcolumn:str,column:str):\n",
        "    \"\"\" return absolute  of integer columns\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column(str):   column name\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with absolute a columns\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.abs(dataframe[column]))\n",
        "    return dataframe_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6296a3d",
      "metadata": {
        "id": "f6296a3d"
      },
      "source": [
        "### raised to power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88eb273c",
      "metadata": {
        "id": "88eb273c"
      },
      "outputs": [],
      "source": [
        "def powernumber(dataframe,newcolumn:str,column1:str,column2:str):\n",
        "    \"\"\" return a column calculate first column power by second column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column1(str): first  column name\n",
        "\n",
        "            column2(str): second  column name\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a column calculate first column power by second column\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe = dataframe.withColumn(newcolumn, F.pow(column1,column2))\n",
        "    return dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f3c000",
      "metadata": {
        "id": "b4f3c000"
      },
      "source": [
        "### Select smallest value out of multiple columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a22b6a",
      "metadata": {
        "id": "05a22b6a"
      },
      "outputs": [],
      "source": [
        "def smallestcolumns( dataframe,newcolumn:str,cols:tuple):\n",
        "    \"\"\" return a  smallest value out of multiple columns \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            cols(tuple):   name of the columns\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  smallest value out of multiple columns \n",
        "            \"\"\"\n",
        "    a=str(cols)\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, eval(\"F.least\"+a))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af4e1980",
      "metadata": {
        "id": "af4e1980"
      },
      "source": [
        "### Select largest value out of multiple columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00a13fc5",
      "metadata": {
        "id": "00a13fc5"
      },
      "outputs": [],
      "source": [
        "def greatestcolumns( dataframe,newcolumn:str,cols:str):\n",
        "    \"\"\" return a  greatest value out of multiple columns \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            cols(tuple):   name of the columns\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  greatest value out of multiple columns\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe = dataframe.withColumn(newcolumn, eval(\"F.greatest\"+str(cols)))\n",
        "    return dataframe\n",
        "                                     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b537949",
      "metadata": {
        "id": "0b537949"
      },
      "source": [
        "### Date & Timestamp Operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53aed9a",
      "metadata": {
        "id": "e53aed9a"
      },
      "source": [
        "#### Convert a string of known format to a date (excludes time information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ba7a6a",
      "metadata": {
        "id": "c1ba7a6a"
      },
      "outputs": [],
      "source": [
        "def formattdate(dataframe,newcolomn:str,column:str,formattime:str):\n",
        "    \"\"\" Convert a string of known format to a date (excludes time information)\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column(str):   name of the columns\n",
        "\n",
        "           formattime(str): standard date format (yyyy-MM-dd HH:mm:ss)\n",
        "          \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a string of known format to a date (excludes time information)\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolomn, F.to_date(column, formattime))\n",
        "    return dataframe_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6363464a",
      "metadata": {
        "id": "6363464a"
      },
      "source": [
        "#### Convert a string of known format to a timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848a6e14",
      "metadata": {
        "id": "848a6e14"
      },
      "outputs": [],
      "source": [
        "def formatotimestamp(dataframe,newcolumn:str,colomn:str,formattime:str):\n",
        "    \"\"\" Convert a string of known format to a timestamp \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column(str):   name of the columns\n",
        "\n",
        "           formattime(str): standard date format (yyyy-MM-dd HH:mm:ss.SSS)\n",
        "          \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a column calculate first column power by second column\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe = dataframe.withColumn(newcolumn, F.to_timestamp(colomn, formattime))\n",
        "    return dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878d28fa",
      "metadata": {
        "id": "878d28fa"
      },
      "source": [
        "#### Get second/minute/hour/day/month/year from date:       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec04e58",
      "metadata": {
        "id": "2ec04e58"
      },
      "outputs": [],
      "source": [
        "def creatyear(dataframe,datecoloums,newcolumn):\n",
        "    \n",
        "    \"\"\" return a column as a year taken from the date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  year from date column\n",
        "            \"\"\"    \n",
        "    from pyspark.sql.functions import year\n",
        "    datafram_new=dataframe.select(year(datecoloums).alias(newcolumn))\n",
        "    return datafram_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f25b7c",
      "metadata": {
        "id": "f8f25b7c"
      },
      "outputs": [],
      "source": [
        "def creatmonth(dataframe,datecoloums,newcolumn):\n",
        "        \n",
        "    \"\"\" return a column as a month taken from the date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  month from date column\n",
        "            \"\"\"    \n",
        "    from pyspark.sql.functions import year\n",
        "    from pyspark.sql.functions import month\n",
        "    datafram_new=dataframe.select(month(datecoloums).alias(newcolumn))\n",
        "    return datafram_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072c67fd",
      "metadata": {
        "id": "072c67fd"
      },
      "outputs": [],
      "source": [
        "def creatdaymonth(dataframe,datecoloums,newcolumn):\n",
        "        \n",
        "    \"\"\" return a column as a day of month taken from the date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  day of month from date column\n",
        "            \"\"\"    \n",
        "\n",
        "    from pyspark.sql.functions import dayofmonth\n",
        "    datafram_new=dataframe.select(dayofmonth(datecoloums).alias(newcolumn))\n",
        "    return datafram_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b648714",
      "metadata": {
        "id": "5b648714"
      },
      "outputs": [],
      "source": [
        "def createhour(dataframe,datecoloums,newcolumn):\n",
        "    \"\"\" return a column as hour taken from the date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  hour from date column\n",
        "            \"\"\"    \n",
        "    from pyspark.sql.functions import hour\n",
        "    datafram_new=dataframe.select(hour(datecoloums).alias(newcolumn))\n",
        "    return datafram_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4858cc1d",
      "metadata": {
        "id": "4858cc1d"
      },
      "outputs": [],
      "source": [
        "def createminute(dataframe,datecoloums,newcolumn):\n",
        "    \"\"\" return a column as minute taken from the date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  minute from date column\n",
        "            \"\"\"    \n",
        "    from pyspark.sql.functions import minute\n",
        "    datafram_new=dataframe.select(minute(datecoloums).alias(newcolumn))\n",
        "    return datafram_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01613f81",
      "metadata": {
        "id": "01613f81"
      },
      "outputs": [],
      "source": [
        "def createsecond(dataframe,datecoloums,newcolumn):\n",
        "    \"\"\" return a column as second taken from the date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a  second from date column\n",
        "            \"\"\"\n",
        "    from pyspark.sql.functions import second\n",
        "    datafram_new=dataframe.select(second(datecoloums).alias(newcolumn))\n",
        "    return datafram_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f825b69",
      "metadata": {
        "id": "0f825b69"
      },
      "source": [
        "#### Add & subtract days\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cb11ae",
      "metadata": {
        "id": "d5cb11ae"
      },
      "outputs": [],
      "source": [
        "def adddatedays(dataframe,newcolumn:str,column,dateadded:int):\n",
        "    \"\"\" add days to date column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            dateadded(int) : number of day add to date column\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a new column that add days to date column\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.date_add(column, dateadded))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2473d7c",
      "metadata": {
        "id": "f2473d7c"
      },
      "outputs": [],
      "source": [
        "def subdatedays(dataframe,newcolumn:str,column,datesub:int):\n",
        "    \"\"\" subtract days from date column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            datesub(int) : number of day subtract from date column\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a new column that subtract days from date column\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    datafram_new = dataframe.withColumn(newcolumn, F.date_sub(column, datesub))\n",
        "    return datafram_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "949c36dc",
      "metadata": {
        "id": "949c36dc"
      },
      "source": [
        "#### Add & Subtract months\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "383822ac",
      "metadata": {
        "id": "383822ac"
      },
      "outputs": [],
      "source": [
        "def addmonth(dataframe,newcolumn,column,dateadded):\n",
        "    \"\"\" add month to date column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "            \n",
        "            datecoloums(str):   name of the columns         \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            dateadded(int) : number of month subtract from date column\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a new column that add month from date column\n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F \n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.add_months(column, dateadded))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a9af7d",
      "metadata": {
        "id": "31a9af7d"
      },
      "source": [
        "#### Get number of days between two dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d500c350",
      "metadata": {
        "id": "d500c350"
      },
      "outputs": [],
      "source": [
        "def daybetween(dataframe,newcolumn,column1,column2):\n",
        "    \"\"\" return a dataframe with diffrent two date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column1(str) : number of month subtract from date column\n",
        "\n",
        "            column2(str) : number of month subtract from date column\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with diffrent two date column \n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.datediff(column1, column2))\n",
        "    return dataframe_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4259d7d7",
      "metadata": {
        "id": "4259d7d7"
      },
      "source": [
        "#### Get number of months between two dates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6a9b47",
      "metadata": {
        "id": "ba6a9b47"
      },
      "outputs": [],
      "source": [
        "def monthbetween(dataframe,newcolumn,column1,column2):\n",
        "    \"\"\" return a dataframe with diffrent two date column \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            column1(str) : number of month subtract from date column\n",
        "\n",
        "            column2(str) : number of month subtract from date column\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with diffrent two date column \n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.months_between(column1, column2))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7659e4ec",
      "metadata": {
        "id": "7659e4ec"
      },
      "source": [
        "### Array Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5450d4",
      "metadata": {
        "id": "9f5450d4"
      },
      "outputs": [],
      "source": [
        "def creatarray(dataframe,newcolumn:str,cols:tuple):\n",
        "    \"\"\" create array from selected columns\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "            cols(tuple(str)) : name of columns\n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with array column from selected columns\n",
        "            \"\"\"\n",
        "    \n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, eval(\"F.array\"+str(cols)))\n",
        "    return dataframe_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a8568c",
      "metadata": {
        "id": "79a8568c"
      },
      "source": [
        "#### Empty Array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab2b20d4",
      "metadata": {
        "id": "ab2b20d4"
      },
      "outputs": [],
      "source": [
        "def creatarray(dataframe,newcolumn):\n",
        "    \"\"\" create  empty array \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with a empty array \n",
        "            \"\"\"\n",
        "    \n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.array([]))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d46cfc9",
      "metadata": {
        "id": "9d46cfc9"
      },
      "source": [
        "#### Array Size/Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e7e9c6",
      "metadata": {
        "id": "23e7e9c6"
      },
      "outputs": [],
      "source": [
        "def sizearray(dataframe,newcolumn:str,columnarray:str):\n",
        "    \"\"\" return size of array\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            newcolumn(str): new column name \n",
        "\n",
        "            columnarray(str) : name of column array\n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe  \n",
        "            \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "    dataframe_new = dataframe.withColumn(newcolumn, F.size(F.col(columnarray)))\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "804d973c",
      "metadata": {
        "id": "804d973c"
      },
      "source": [
        "## Aggregation Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783995a6",
      "metadata": {
        "id": "783995a6"
      },
      "source": [
        "#### Row Count       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb9c23e",
      "metadata": {
        "id": "3bb9c23e"
      },
      "outputs": [],
      "source": [
        "def valuecount(dataframe,columname:str)->int:\n",
        "    \"\"\" return count of selected column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "\n",
        "\n",
        "            columname(str) : name of column \n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            count (int) :  count of selected column \n",
        "            \"\"\"\n",
        "    count=dataframe.select(columname).count()\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b62eab8",
      "metadata": {
        "id": "1b62eab8"
      },
      "outputs": [],
      "source": [
        "def meancolumn(dataframe,groupbycolumns:tuple,meancolumns:str):\n",
        "    \"\"\" return mean of selected column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            groupbycolumns(tuple) : name of columns \n",
        "\n",
        "            meancolumns(str):mean column\n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with mean of selected column\n",
        "            \n",
        "            \"\"\"\n",
        "\n",
        "    dataframe_new=dataframe.groupBy(groupbycolumns).mean(meancolumns)\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b56c0cd",
      "metadata": {
        "id": "2b56c0cd"
      },
      "outputs": [],
      "source": [
        "def maxncloumn(dataframe,groupbycolumns:tuple,maxcolumns:str):\n",
        "    \"\"\" return max of selected column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            groupbycolumns(tuple) : name of columns \n",
        "\n",
        "            maxcolumns(str):mean column\n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with max of selected column\n",
        "            \n",
        "            \"\"\"\n",
        "    dataframe_new=dataframe.groupBy(groupbycolumns).max(maxcolumns)\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f23dc8",
      "metadata": {
        "id": "b0f23dc8"
      },
      "outputs": [],
      "source": [
        "def mincloumn(dataframe,groupbycolumns:tuple,mincloumns:str):\n",
        "    \"\"\" return min of selected column\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            groupbycolumns(tuple) : name of columns \n",
        "\n",
        "            mincloumns(str):mean column\n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with min of selected column\n",
        "            \n",
        "            \"\"\"\n",
        "    dataframe_new=dataframe.groupBy(groupbycolumns).min(mincloumns)\n",
        "    return dataframe_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986b8ecd",
      "metadata": {
        "id": "986b8ecd"
      },
      "source": [
        "### Advanced Operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark Repartition provides a full shuffling of data"
      ],
      "metadata": {
        "id": "7rSh7c0rTD0E"
      },
      "id": "7rSh7c0rTD0E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57400b4e",
      "metadata": {
        "id": "57400b4e"
      },
      "outputs": [],
      "source": [
        "def repartition(dataframe,column:str):\n",
        "    \"\"\" return dataframe  with a new repartition\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            column(str) : name of columns \n",
        "\n",
        "          \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe  with a new repartition\n",
        "            \n",
        "            \"\"\"\n",
        "\n",
        "    dataframe_new = dataframe.repartition(dataframe[column])\n",
        "    return dataframe_new\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def casewhen(casewhenexp:str,dataframe): \n",
        "  \"\"\" return dataframe with only words and space between.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            casewhenexp(str):  case when expression\n",
        "\n",
        "  \n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new dataframe with operation on column\n",
        "\n",
        "        Example:\n",
        "        --------------\n",
        "\n",
        "        dataframe_new = dataframe.withColumn(\"mood\",expr(\"CASE WHEN EST2 in('4','5') THEN 'chill' WHEN EST2 in('1','2') THEN 'highstrung' ELSE 'neutral' END AS mood\"))\n",
        "\n",
        "            \n",
        "    \"\"\"  \n",
        "  from pyspark.sql.functions import expr\n",
        "\n",
        "  dataframe_new = dataframe.withColumn(\"vert\",expr(casewhenexp))\n",
        "  return dataframe_new"
      ],
      "metadata": {
        "id": "whNnIiKNomuD"
      },
      "id": "whNnIiKNomuD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "179ebf3a",
      "metadata": {
        "id": "179ebf3a"
      },
      "source": [
        "# machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Prepration"
      ],
      "metadata": {
        "id": "iOt7qs_vo7yq"
      },
      "id": "iOt7qs_vo7yq"
    },
    {
      "cell_type": "code",
      "source": [
        "def dependent_var(dataframe,dependent_var):\n",
        "  \"\"\" return  a  dataframe with numeric dependent variable \n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "\n",
        "            columnname(str):encoding the selected dependent column\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new  dataframe with numeric dependent variable \n",
        "      \"\"\"\n",
        "  from pyspark.sql.types import StringType \n",
        "  renamed = dataframe.withColumnRenamed(dependent_var,'label')\n",
        "  # Make sure dependent variable is numeric and change if it's not\n",
        "  if str(renamed.schema['label'].dataType) != 'IntegerType':\n",
        "      renamed = renamed.withColumn(\"label\", renamed[\"label\"].cast(FloatType()))\n",
        "  return renamed"
      ],
      "metadata": {
        "id": "JPcS1tTnoocT"
      },
      "id": "JPcS1tTnoocT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dummyencoding(dataframe,column:str): \n",
        "  \"\"\" return  a  dataframe with encoding the selected column\n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "\n",
        "            columnname(str):encoding the selected column\n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_new (:class:`DataFrame) : new encoding dataframe \n",
        "            \n",
        "\n",
        "  \"\"\"\n",
        "  import pyspark.sql.functions as F \n",
        "  categ = dataframe.select(column).distinct().rdd.flatMap(lambda x:x).collect()\n",
        "  exprs = [F.when(F.col(column) == cat,1).otherwise(0)\\\n",
        "              .alias(str(cat)) for cat in categ]\n",
        "  dataframe_new = dataframe.select(dataframe.columns+exprs).drop(column)\n",
        "  return dataframe_new"
      ],
      "metadata": {
        "id": "JYe3ghngo9k-"
      },
      "id": "JYe3ghngo9k-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73efc367",
      "metadata": {
        "id": "73efc367"
      },
      "outputs": [],
      "source": [
        "def MLClassifierDFPrep(dataframe,input_columns:list,dependent_var:str,treat_outliers:bool=True,treat_neg_values:bool=True):\n",
        "    \"\"\" return a rescaling dataframe with data prepration ( checking skewness and outliers)\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            dependent_var(str) : name of dependent columns \n",
        "\n",
        "            treat_outliers(bool) : modify dataframe for outliers\n",
        "            \n",
        "            treat_neg_values(bool) : modify dataframe for negative values\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            final_data (:class:`DataFrame) : the rescaling dataframe  with data prepration \n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "\n",
        "    from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
        "    from pyspark.sql.types import StringType \n",
        "    from pyspark.sql.functions import skewness,log,when,array,array_min,exp\n",
        "    renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) \n",
        "    indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") \n",
        "    indexed = indexer.fit(renamed).transform(renamed)\n",
        "\n",
        "    numeric_inputs = []\n",
        "    string_inputs = []\n",
        "    for column in input_columns:\n",
        "        if str(indexed.schema[column].dataType) == 'StringType':\n",
        "            indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
        "            indexed = indexer.fit(indexed).transform(indexed)\n",
        "            new_col_name = column+\"_num\"\n",
        "            string_inputs.append(new_col_name)\n",
        "        else:\n",
        "            numeric_inputs.append(column)\n",
        "            \n",
        "    if treat_outliers == True:\n",
        "        print(\"We are correcting for non normality now!\")\n",
        "        d = {}\n",
        "        for col in numeric_inputs: \n",
        "            d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) \n",
        "        for col in numeric_inputs:\n",
        "            skew = indexed.agg(skewness(indexed[col])).collect() \n",
        "            skew = skew[0][0]\n",
        "            if skew > 1:\n",
        "                indexed = indexed.withColumn(col, \\\n",
        "                log(when(df[col] < d[col][0],d[col][0])\\\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "                .otherwise(indexed[col] ) +1).alias(col))\n",
        "                print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
        "            elif skew < -1:\n",
        "                indexed = indexed.withColumn(col, \\\n",
        "                exp(when(df[col] < d[col][0],d[col][0])\\\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "                .otherwise(indexed[col] )).alias(col))\n",
        "                print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
        "\n",
        "            \n",
        "    minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) \n",
        "    min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) \n",
        "    df_minimum = min_array.select(array_min(min_array.mins)).collect() \n",
        "    df_minimum = df_minimum[0][0] \n",
        "\n",
        "    features_list = numeric_inputs + string_inputs\n",
        "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
        "    output = assembler.transform(indexed).select('features','label')\n",
        "\n",
        "    if df_minimum < 0:\n",
        "        print(\" \")\n",
        "        print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
        "        print(\" \")\n",
        "    \n",
        "    if treat_neg_values == True:\n",
        "        print(\"You have opted to correct that by rescaling all your features to a range of 0 to 1\")\n",
        "        print(\" \")\n",
        "        print(\"We are rescaling you dataframe....\")\n",
        "        scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "        scalerModel = scaler.fit(output)\n",
        "\n",
        "        scaled_data = scalerModel.transform(output)\n",
        "        final_data = scaled_data.select('label','scaledFeatures')\n",
        "        final_data = final_data.withColumnRenamed('scaledFeatures','features')\n",
        "        print(\"Done!\")\n",
        "\n",
        "    else:\n",
        "        print(\"You have opted not to correct that therefore you will not be able to use to Naive Bayes classifier\")\n",
        "        print(\"We will return the dataframe unscaled.\")\n",
        "        final_data = output\n",
        "    \n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ee6349",
      "metadata": {
        "id": "03ee6349"
      },
      "outputs": [],
      "source": [
        "def mainclustring(dataframe,inputCols:list):\n",
        "    \"\"\" return a vectorize dataframe \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            dataframe_kmeans (:class:`DataFrame) : the vectorize dataframe  \n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "\n",
        "  from pyspark.ml.feature import VectorAssembler\n",
        "  vecAssembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
        "  dataframe_kmeans = vecAssembler.transform(dataframe)\n",
        "  return dataframe_kmeans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8ee408",
      "metadata": {
        "id": "ac8ee408"
      },
      "source": [
        "### min/max scaler & Standard Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56811f85",
      "metadata": {
        "id": "56811f85"
      },
      "outputs": [],
      "source": [
        "def StandardScalerclustring(dataframe,inputCol:list):\n",
        "    \"\"\" return a scaling dataframe \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            scale_df (:class:`DataFrame) : the scaling dataframe  \n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "    from pyspark.ml.feature import StandardScaler\n",
        "    tr=mainclustring(dataframe,inputCol)\n",
        "    scaler = StandardScaler(inputCol=\"features\",outputCol=\"scaled\",withStd=True,withMean=False)\n",
        "    scale_df=scaler.fit(tr).transform(tr)\n",
        "    scale_df=scale_df.drop(\"features\")\n",
        "    scale_df=scale_df.withColumnRenamed(\"scaled\",\"features\")\n",
        "    return scale_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b4db8d",
      "metadata": {
        "id": "62b4db8d"
      },
      "outputs": [],
      "source": [
        "def minmaxscaling(dataframe,input_columns): \n",
        "  \"\"\" return a scaling dataframe \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            scale_df (:class:`DataFrame) : the scaling dataframe  \n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "  from pyspark.ml.feature import MinMaxScaler\n",
        "  tr=mainclustring(dataframe,input_columns)\n",
        "  scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"scaled\")\n",
        "  scale_df=scaler.fit(tr).transform(tr)\n",
        "  scale_df=scale_df.drop(\"features\")\n",
        "  scale_df=scale_df.withColumnRenamed(\"scaled\",\"features\")\n",
        "  return scale_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfc673c7",
      "metadata": {
        "id": "bfc673c7"
      },
      "source": [
        "### clustering with kmeans "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dfe4535",
      "metadata": {
        "id": "0dfe4535"
      },
      "outputs": [],
      "source": [
        "def keamnsclustering(kmeansmethod:str,scaleingmethod:str,dataframe,inputCols:list,k:int,seed:int,itr:int)->list:\n",
        "    \"\"\" return  silhouette,center of kmeans\n",
        "             \n",
        "        Parameters:\n",
        "        --------------\n",
        "            \n",
        "            kmeansmethod(str) : selecct   KMeans or BisectingKMeans method\n",
        "                \n",
        "            scaleingmethod(str): selecct   MinMaxScaler or StandardScaler method\n",
        "\n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            k(int) : number of center \n",
        "            \n",
        "            seed(int) : seed of model\n",
        "\n",
        "            itr(int) : number of iteration of model\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            silhcent (list) : return a tuple with silhouette,center of kmeans ,prediction dataframe \n",
        "              \n",
        "            \n",
        "           \"\"\" \n",
        "    silhcent=list()       \n",
        "    from pyspark.ml.clustering import BisectingKMeans\n",
        "    import numpy as np\n",
        "    from pyspark.ml.feature import StandardScaler\n",
        "    from pyspark.ml.clustering import KMeans\n",
        "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "    if scaleingmethod==\"MinMaxScaler\":\n",
        "      tr=minmaxscaling(dataframe,inputCols)\n",
        "    elif scaleingmethod==\"StandardScaler\":\n",
        "      tr=StandardScalerclustring(dataframe,inputCols)\n",
        "    for k in np.arange(2,k+1).tolist():\n",
        "      if kmeansmethod == \"KMeans\": \n",
        "        Means=KMeans()\n",
        "      elif kmeansmethod==\"BisectingKMeans\":\n",
        "        Means=BisectingKMeans()\n",
        "      Means.setMaxIter(itr)\n",
        "      Means.setK(k)\n",
        "      Means.setSeed(seed)\n",
        "      Model = Means.fit(tr)\n",
        "      pred = Model.transform(tr)\n",
        "      Evaluator = ClusteringEvaluator()\n",
        "      silhouette=Evaluator.evaluate(pred)\n",
        "      center = Model.clusterCenters()\n",
        "      s=(silhouette,center,pred)\n",
        "      silhcent.append(s)\n",
        "    return silhcent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### predictions of kmeans method "
      ],
      "metadata": {
        "id": "fFC7deY4F-A8"
      },
      "id": "fFC7deY4F-A8"
    },
    {
      "cell_type": "code",
      "source": [
        "def predictions(kmeansmethod:str,scaleingmethod:str,dataframe,inputCols:list,k:int,seed:int,itr:int,numberbest:int):  \n",
        "  \"\"\" return  perdiction dataframe\n",
        "             \n",
        "        Parameters:\n",
        "        --------------\n",
        "            \n",
        "            kmeansmethod(str) : selecct   KMeans or BisectingKMeans method\n",
        "                \n",
        "            scaleingmethod(str): selecct   MinMaxScaler or StandardScaler method\n",
        "\n",
        "            dataframe(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            k(int) : number of center \n",
        "            \n",
        "            seed(int) : seed of model\n",
        "\n",
        "            itr(int) : number of iteration of model\n",
        "\n",
        "            numberbest(int): number of best silhouette kmeans  calculated\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            scale_df (:class:`DataFrame) : the predictions dataframe  \n",
        "              \n",
        "            \n",
        "           \"\"\" \n",
        "  predictions=keamnsclustering(kmeansmethod,scaleingmethod,dataframe,inputCols,k,seed,itr)[numberbest-2][2]\n",
        "  df_predictions=predictions.groupBy(\"prediction\").agg(min(predictions.BALANCE).alias(\"Min BALANCE\"),max(predictions.BALANCE).alias(\"Max BALANCE\"))\n",
        "  return df_predictions\n"
      ],
      "metadata": {
        "id": "Je6ix6TAF9IA"
      },
      "id": "Je6ix6TAF9IA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA for nlp clusstering "
      ],
      "metadata": {
        "id": "602tNU4pbGDA"
      },
      "id": "602tNU4pbGDA"
    },
    {
      "cell_type": "code",
      "source": [
        "def regremvec(inputCols:list,dataframe)->tuple: \n",
        "  \"\"\" regex tokenizer and vector dataframe.\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "             inputCol(list):  column name \n",
        "\n",
        "             \n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            regremvec (tuple) : tuple with three main dataframe(raw_words,df_vect,cvmodel)\n",
        "            \n",
        "    \"\"\"\n",
        "  # Tokenize\n",
        "  raw_words=dataframe\n",
        "  for count, inputCol in enumerate(inputCols):\n",
        "    inputCol=inputCol\n",
        "    outputCol=\"words\"+str(count+1)\n",
        "    regex_tokenizer = RegexTokenizer(inputCol=inputCol, outputCol=outputCol, pattern=\"\\\\W\")\n",
        "    raw_words = regex_tokenizer.transform(raw_words)\n",
        " # Remove Stop words\n",
        "  words_df=raw_words\n",
        "  for count, inputCol in enumerate(inputCols):\n",
        "    inputCol=\"words\"+str(count+1)\n",
        "    outputCol=\"filtered\"+str(count+1)\n",
        "  \n",
        "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "    words_df = remover.transform(words_df)\n",
        "\n",
        "  df_vect=words_df\n",
        "  for count, inputCol in enumerate(inputCols):\n",
        "    inputCol=\"filtered\"+str(count+1)\n",
        "    outputCol=\"features\"+str(count+1)    \n",
        "    cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
        "    cvmodel = cv.fit(words_df)\n",
        "    df_vect = cvmodel.transform(df_vect)\n",
        "    regremvec=(raw_words,df_vect,cvmodel)\n",
        "  return regremvec\n",
        "\n",
        "  # Zero Index Label Column\n"
      ],
      "metadata": {
        "id": "yOyuXpq7vscS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yOyuXpq7vscS"
    },
    {
      "cell_type": "code",
      "source": [
        "def LDAplot(inputCol:list,dataframe,kmax:int):\n",
        "  \"\"\" LDA datframe with plot\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "            \n",
        "                       \n",
        "            inputCol(list):  column name \n",
        "\n",
        "            \n",
        "            kmax(int): maximum k of LDA\n",
        "\n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            lda (:class:`DataFrame) : lda datframe\n",
        "            \"\"\"\n",
        "  import matplotlib.pyplot as plt\n",
        "  from mpl_toolkits.mplot3d import Axes3D\n",
        "  import numpy as np\n",
        "  df_vect=regremvec(inputCol,dataframe)[1]\n",
        "  ll = np.zeros(kmax)\n",
        "  lp = np.zeros(kmax)\n",
        "  for k in np.arange(2,kmax).tolist():\n",
        "      lda = LDA(k=k, maxIter=10)\n",
        "      model = lda.fit(df_vect)\n",
        "      ll[k] = model.logLikelihood(df_vect)\n",
        "      lp[k] = model.logPerplexity(df_vect)\n",
        "      \n",
        "  fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
        "  ax.plot(np.arange(2,kmax).tolist(),ll[2:kmax])\n",
        "  ax.set_xlabel('k')\n",
        "  ax.set_ylabel('ll')\n",
        "\n",
        "  fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
        "  ax.plot(np.arange(2,kmax).tolist(),lp[2:kmax])\n",
        "  ax.set_xlabel('k')\n",
        "  ax.set_ylabel('lp')\n",
        "  return lda"
      ],
      "metadata": {
        "id": "Z3t0G9Nvy4fI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Z3t0G9Nvy4fI"
    },
    {
      "cell_type": "code",
      "source": [
        "def topic(inputCol:list,dataframe,k:int,maxIter:int):  \n",
        "  \"\"\" return  a topic dataframe \n",
        "            \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe           \n",
        "\n",
        "            inputCol(str):input column list\n",
        "        Returns:\n",
        "        --------------\n",
        "            results (:class:`DataFrame) : topic dataframe \n",
        "            \n",
        "\n",
        "            \"\"\"\n",
        "  # Make predictions\n",
        "  # Trains a LDA model.\n",
        "  lda = LDA(k=k, maxIter=maxIter)\n",
        "  df_vect=regremvec(inputCol,dataframe)[0]\n",
        "  model = lda.fit(df_vect)\n",
        "  transformed = model.transform(df_vect)\n",
        "  transformed.toPandas()\n",
        "\n",
        "  # Convert topicdistribution col from vector to array\n",
        "  # We need to create a udf for this one\n",
        "  to_array = udf(lambda x: x.toArray().tolist(), ArrayType(DoubleType()))\n",
        "  recommendations = transformed.withColumn('array', to_array('topicDistribution'))\n",
        "\n",
        "  # Find the best topic value that we will call \"max\"\n",
        "  max_vals = recommendations.withColumn(\"max\",array_max(\"array\"))\n",
        "\n",
        "  # Find the index of the max value found above which translates to our topic!\n",
        "  argmaxUdf = udf(lambda x,y: [i for i, e in enumerate(x) if e==y ])\n",
        "  results = max_vals.withColumn('topic', argmaxUdf(max_vals.array,max_vals.max))\n",
        "  return results"
      ],
      "metadata": {
        "id": "Ha39OS5N1Jy6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ha39OS5N1Jy6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gaussian Mixture Modeling"
      ],
      "metadata": {
        "id": "1A3YFE-2sGct"
      },
      "id": "1A3YFE-2sGct"
    },
    {
      "cell_type": "code",
      "source": [
        "def Gaussianplot(dataframe,input_columns:list,kmax:int,maxIter:int,seed:int):\n",
        "  \"\"\" return a new vectorize dataframe  with plot of k\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "\n",
        "            input_columns(list) :input columns     \n",
        "            \n",
        "            kmax(int) : maximum of k \n",
        "\n",
        "            maxIter(int) : maximum iteration \n",
        "\n",
        "            seed(int) : seed of Gaussian Mixture cluss cluster model\n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            final_df (:class:`DataFrame) : new vectorize dataframe  with plot of k\n",
        "            \"\"\"\n",
        "  vecAssembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
        "  final_df = vecAssembler.transform(dataframe)\n",
        "  ll = np.zeros(kmax)\n",
        "  for k in np.arange(2,kmax).tolist():\n",
        "      gm = GaussianMixture(k=k, tol=0.0001,maxIter=maxIter, seed=seed)\n",
        "      model = gm.fit(final_df)\n",
        "      summary = model.summary\n",
        "      ll[k] = summary.logLikelihood\n",
        "  fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
        "  ax.plot(np.arange(2,kmax).tolist(),ll[2:kmax])\n",
        "  ax.set_xlabel('k')\n",
        "  ax.set_ylabel('ll')\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "mrGFJmgyfSpg"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mrGFJmgyfSpg"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def transformed(dataframe,k_opt:int,input_columns:list,kmax:int,maxIter:int,seed:int)->tuple:\n",
        "  \"\"\" return a new transformed dataframe  with additional Gaussian information\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : select dataframe  \n",
        "\n",
        "            input_columns(list) :input columns  \n",
        "\n",
        "            k_opt(int) : optimal k   \n",
        "            \n",
        "            kmax(int) : maximum of k \n",
        "\n",
        "            maxIter(int) : maximum iteration \n",
        "\n",
        "            seed(int) : seed of Gaussian Mixture cluss cluster model\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            tr_all (tuple) : new transformed dataframe  with additional Gaussian information and summary of all\n",
        "            \"\"\"\n",
        "  final_df=Gaussianplot(dataframe,input_columns,kmax,maxIter,seed)\n",
        "  gm = GaussianMixture(k=k_opt, maxIter=maxIter, seed=seed)\n",
        "  model = gm.fit(final_df)\n",
        "\n",
        "  summary = model.summary\n",
        "  all_summery={}\n",
        "  all_summery[\"Clusters\"]=summary.k\n",
        "  all_summery[\"Cluster Sizes\"]=summary.clusterSizes\n",
        "  all_summery[\"Log Likelihood\"]=summary.logLikelihood\n",
        "\n",
        "  weights = model.weights\n",
        "  all_summery[\"Model Weights\"]=len(weights)\n",
        "\n",
        "  all_summery[\"Means\"]= model.gaussiansDF.select(\"mean\").head()\n",
        "\n",
        "  all_summery[\"Cov\"]=model.gaussiansDF.select(\"cov\").head()\n",
        "\n",
        "  transformed = model.transform(final_df)\n",
        "  limited =transformed.filter(\"prediction == 0\")\n",
        "  aggregates = limited.summary(\"min\", \"mean\", \"max\")\n",
        "  all_summery[\"Total Cases in this Cluster\"]=limited.count()\n",
        "  tr_all=(transformed,all_summery,aggregates)\n",
        "  return tr_all"
      ],
      "metadata": {
        "id": "yNX3Qq74g4pi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yNX3Qq74g4pi"
    },
    {
      "cell_type": "markdown",
      "id": "jZU7d0x6j6zQ",
      "metadata": {
        "id": "jZU7d0x6j6zQ"
      },
      "source": [
        "# classification and regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8m83HN9VyqNt"
      },
      "id": "8m83HN9VyqNt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2dGTnxyqeH"
      },
      "source": [
        "### classification \n",
        "\n"
      ],
      "id": "Fr2dGTnxyqeH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixYwQZs3TjQ9"
      },
      "source": [
        "#### MultilayerPerceptronClassifier"
      ],
      "id": "ixYwQZs3TjQ9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKI-AdXwTjwx"
      },
      "outputs": [],
      "source": [
        "def MultilayerPerceptronClassifiermodel(features,classes,folds,train,test,input_columns):\n",
        "    \"\"\" return a Multilayer Perceptron Classifier accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(str) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return a Multilayer Perceptron Classifier  accuracy\n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    classifier=MultilayerPerceptronClassifier()\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(classifier)\n",
        "    \n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,train):\n",
        "              \n",
        "            # specify layers for the neural network:\n",
        "            # input layer of size features, two intermediate of features+1 and same size as features\n",
        "            # and output of size number of classes\n",
        "            # Note: crossvalidator cannot be used here\n",
        "            features_count = len(features[0][0])\n",
        "            layers = [features_count, features_count+1, features_count, classes]\n",
        "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "            fitModel = MPC_classifier.fit(train)\n",
        "            return fitModel\n",
        "                      \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,train)\n",
        "  \n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "rKI-AdXwTjwx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGSeHHHoZmbU"
      },
      "source": [
        "#### OneVsRest"
      ],
      "id": "VGSeHHHoZmbU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4543b496"
      },
      "outputs": [],
      "source": [
        "def OneVsRestmodel(features,classes,folds,train,test,input_columns):\n",
        "    \"\"\" return a One Vs Rest accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(str) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return One Vs Rest  accuracy\n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "    from pyspark.ml.classification import OneVsRest,LogisticRegression\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    classifier=OneVsRest()\n",
        "    Mtype=\"OneVsRest\"\n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,train):\n",
        "        \n",
        "        if Mtype == \"OneVsRest\":\n",
        "            # instantiate the base classifier.\n",
        "            lr = LogisticRegression()\n",
        "            # instantiate the One Vs Rest Classifier.\n",
        "            OVRclassifier = OneVsRest(classifier=lr)\n",
        "#             fitModel = OVRclassifier.fit(train)\n",
        "            # Add parameters of your choice here:\n",
        "            paramGrid = ParamGridBuilder() \\\n",
        "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "                .build()\n",
        "            #Cross Validator requires the following parameters:\n",
        "            crossval = CrossValidator(estimator=OVRclassifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=2) # 3 is best practice\n",
        "            # Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "\n",
        "    \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,train)\n",
        "    \n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "        \n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            # Extract list of binary models\n",
        "            models = BestModel.models\n",
        "            for model in models:\n",
        "                print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept,'\\033[1m' + '\\nCoefficients:'+ '\\033[0m',model.coefficients)\n",
        "\n",
        "\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "4543b496"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDBEV4lHc5Mb"
      },
      "source": [
        "#### LogisticRegression"
      ],
      "id": "mDBEV4lHc5Mb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF1pY4Wsc0I4"
      },
      "outputs": [],
      "source": [
        "def LogisticRegressionmodel(features,classes,folds,train,test,input_columns) :\n",
        "    \"\"\" return Logistic Regression accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(str) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Logistic Regression  accuracy\n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "    from pyspark.ml.classification import LogisticRegression\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    classifier=LogisticRegression()\n",
        "    def FindMtype(classifier):\n",
        "    \n",
        "        M = classifier\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(classifier)\n",
        "    \n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
        "        \n",
        "\n",
        "\n",
        "       \n",
        "  \n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"LogisticRegression\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
        "                             .addGrid(classifier.maxIter, [10, 15,20])\n",
        "                             .build())\n",
        "                \n",
        "\n",
        "            \n",
        "            #Cross Validator requires all of the following parameters:\n",
        "            crossval = CrossValidator(estimator=classifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 + is best practice\n",
        "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "    \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
        "    \n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "        \n",
        "       \n",
        "        # Print the coefficients\n",
        "        if Mtype in(\"LogisticRegression\"):\n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            print(\"Intercept: \" + str(BestModel.interceptVector))\n",
        "            print('\\033[1m' + \" Top 20 Coefficients\"+ '\\033[0m')\n",
        "            print(\"You should compares these relative to eachother\")\n",
        "            # Convert from numpy array to list\n",
        "            coeff_array = BestModel.coefficientMatrix.toArray()\n",
        "            coeff_scores = []\n",
        "            for x in coeff_array[0]:\n",
        "                coeff_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
        "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
        "            # Save the coefficient values and the models\n",
        "            global LR_coefficients\n",
        "            LR_coefficients = BestModel.coefficientMatrix.toArray()\n",
        "            global LR_BestModel\n",
        "            LR_BestModel = BestModel\n",
        "\n",
        "        \n",
        "   \n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "WF1pY4Wsc0I4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU84O559jU_-"
      },
      "source": [
        "#### Naive Bayes model"
      ],
      "id": "IU84O559jU_-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT2MH-s9jTk7"
      },
      "outputs": [],
      "source": [
        "def NaiveBayesmodel(features,classes,folds,train,test,input_columns) :\n",
        "    \"\"\" return Naive Bayes accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(str) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Naive Bayes  accuracy\n",
        "              \n",
        "            \"\"\"    \n",
        "    from pyspark.ml.classification import NaiveBayes\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    classifier=NaiveBayes()\n",
        "    \n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(classifier)\n",
        "    \n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
        "        \n",
        "        \n",
        "      \n",
        "            if Mtype in(\"NaiveBayes\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "                             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
        "                             .build())\n",
        "                \n",
        "            \n",
        "            \n",
        "          \n",
        "            #Cross Validator requires all of the following parameters:\n",
        "            crossval = CrossValidator(estimator=classifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 + is best practice\n",
        "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "    \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
        "    \n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "        \n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "        # Print the coefficients\n",
        "       \n",
        "        if Mtype in(\"LinearSVC\"):\n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            print(\"Intercept: \" + str(BestModel.intercept))\n",
        "            print('\\033[1m' + \"Top 20 Coefficients\"+ '\\033[0m')\n",
        "            print(\"You should compares these relative to eachother\")\n",
        "#             print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
        "            coeff_array = BestModel.coefficients.toArray()\n",
        "            coeff_scores = []\n",
        "            for x in coeff_array:\n",
        "                coeff_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
        "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
        "            # Save the coefficient values and the models\n",
        "            global LSVC_coefficients\n",
        "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
        "            global LSVC_BestModel\n",
        "            LSVC_BestModel = BestModel\n",
        "        \n",
        "   \n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "JT2MH-s9jTk7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDOKA4v8_GE1"
      },
      "source": [
        "#### Random Forest Classifier model"
      ],
      "id": "yDOKA4v8_GE1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeNGHlfdlLrh"
      },
      "outputs": [],
      "source": [
        "def RandomForestClassifiermodel(features,classes,folds,train,test,input_columns) :\n",
        "    \"\"\" return Random Forest Classifier accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(str) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Random Forest Classifier  accuracy\n",
        "              \n",
        "            \"\"\"\n",
        "    from pyspark.ml.classification import RandomForestClassifier\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    classifier=RandomForestClassifier()\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(classifier)\n",
        "    \n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
        "        \n",
        "       \n",
        "        \n",
        "  \n",
        "\n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"RandomForestClassifier\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
        "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
        "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
        "                             .build())\n",
        "                \n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "            #Cross Validator requires all of the following parameters:\n",
        "            crossval = CrossValidator(estimator=classifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 + is best practice\n",
        "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "    \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
        "    \n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype,\" Top 20 Feature Importances\"+ '\\033[0m')\n",
        "            print(\"(Scores add up to 1)\")\n",
        "            print(\"Lowest score is the least important\")\n",
        "            print(\" \")\n",
        "            featureImportances = BestModel.featureImportances.toArray()\n",
        "            # Convert from numpy array to list\n",
        "            imp_scores = []\n",
        "            for x in featureImportances:\n",
        "                imp_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "            print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
        "            \n",
        "\n",
        "            if Mtype in(\"RandomForestClassifier\"):\n",
        "                global RF_featureimportances\n",
        "                RF_featureimportances = BestModel.featureImportances.toArray()\n",
        "                global RF_BestModel\n",
        "                RF_BestModel = BestModel\n",
        "\n",
        "  \n",
        "        \n",
        "   \n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "qeNGHlfdlLrh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l92A9IEkwWsG"
      },
      "source": [
        "#### Decision Tree Classifier model"
      ],
      "id": "l92A9IEkwWsG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOVS6fNfwVXq"
      },
      "outputs": [],
      "source": [
        "def DecisionTreeClassifiermodel(features,classes,folds,train,test,input_columns) :\n",
        "    \"\"\" return  Decision Tree Classifier accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(str) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Decision Tree Classifier  accuracy\n",
        "              \n",
        "            \"\"\"\n",
        "    from pyspark.ml.classification import DecisionTreeClassifier\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    classifier=DecisionTreeClassifier()\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(classifier)\n",
        "    \n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
        "        \n",
        "       \n",
        "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
        "  \n",
        "\n",
        "            \n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"DecisionTreeClassifier\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
        "                             .build())\n",
        "            \n",
        "            #Cross Validator requires all of the following parameters:\n",
        "            crossval = CrossValidator(estimator=classifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 + is best practice\n",
        "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "    \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
        "    \n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "\n",
        "\n",
        "            # FEATURE IMPORTANCES\n",
        "            # Estimate of the importance of each feature.\n",
        "            # Each feature’s importance is the average of its importance across all trees \n",
        "            # in the ensemble The importance vector is normalized to sum to 1. \n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype,\" Top 20 Feature Importances\"+ '\\033[0m')\n",
        "            print(\"(Scores add up to 1)\")\n",
        "            print(\"Lowest score is the least important\")\n",
        "            print(\" \")\n",
        "            featureImportances = BestModel.featureImportances.toArray()\n",
        "            # Convert from numpy array to list\n",
        "            imp_scores = []\n",
        "            for x in featureImportances:\n",
        "                imp_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "            print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
        "            \n",
        "            # Save the feature importance values and the models\n",
        "            if Mtype in(\"DecisionTreeClassifier\"):\n",
        "                global DT_featureimportances\n",
        "                DT_featureimportances = BestModel.featureImportances.toArray()\n",
        "                global DT_BestModel\n",
        "                DT_BestModel = BestModel\n",
        "\n",
        "\n",
        "        # Print the coefficients\n",
        "\n",
        "        # Print the Coefficients\n",
        "\n",
        "        \n",
        "   \n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "WOVS6fNfwVXq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAByl7t_EuhB"
      },
      "source": [
        "#### GBTClassifier"
      ],
      "id": "QAByl7t_EuhB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97xjrmO61OHp"
      },
      "outputs": [],
      "source": [
        "def GBTClassifiermodel(features,classes,folds,train,test,input_columns):\n",
        "    \"\"\" return GBT Classifier accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(str) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return GBT Classifier  accuracy\n",
        "              \n",
        "            \"\"\"\n",
        "    from pyspark.ml.classification import GBTClassifier\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    classifier=GBTClassifier()\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(classifier)\n",
        "    \n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
        "        \n",
        "        \n",
        "  \n",
        "\n",
        "        if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2: # These classifiers currently only accept binary classification\n",
        "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
        "            return\n",
        "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
        "  \n",
        "\n",
        "            if Mtype in(\"GBTClassifier\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
        "                             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
        "                             .build())\n",
        "                \n",
        "\n",
        "\n",
        "            \n",
        "            #Cross Validator requires all of the following parameters:\n",
        "            crossval = CrossValidator(estimator=classifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 + is best practice\n",
        "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "    \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
        "    \n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # FEATURE IMPORTANCES\n",
        "            # Estimate of the importance of each feature.\n",
        "            # Each feature’s importance is the average of its importance across all trees \n",
        "            # in the ensemble The importance vector is normalized to sum to 1. \n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype,\" Top 20 Feature Importances\"+ '\\033[0m')\n",
        "            print(\"(Scores add up to 1)\")\n",
        "            print(\"Lowest score is the least important\")\n",
        "            print(\" \")\n",
        "            featureImportances = BestModel.featureImportances.toArray()\n",
        "            # Convert from numpy array to list\n",
        "            imp_scores = []\n",
        "            for x in featureImportances:\n",
        "                imp_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "            print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
        "            \n",
        "            # Save the feature importance values and the models\n",
        "\n",
        "            if Mtype in(\"GBTClassifier\"):\n",
        "                global GBT_featureimportances\n",
        "                GBT_featureimportances = BestModel.featureImportances.toArray()\n",
        "                global GBT_BestModel\n",
        "                GBT_BestModel = BestModel\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "   \n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "97xjrmO61OHp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtYKB_L0umn9"
      },
      "source": [
        "#### Linear SVC model"
      ],
      "id": "PtYKB_L0umn9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV-tNyktsvnz"
      },
      "outputs": [],
      "source": [
        "def LinearSVCmodel(features,classes,folds,train,test,input_columns):\n",
        "    \"\"\" return Linear SVC accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            features(:class:`DataFrame) : features column       \n",
        "            \n",
        "            classes(int) : number of column target \n",
        "\n",
        "            folds(int) : estimate the skill of the model on new data \n",
        "\n",
        "            train(:class:`DataFrame) : train dataframe \n",
        "            \n",
        "            test(:class:`DataFrame) :   test dataframe \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Linear SVC  accuracy\n",
        "              \n",
        "            \"\"\"\n",
        "    from pyspark.ml.classification import LinearSVC\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    classifier=LinearSVC()\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(classifier)\n",
        "    \n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
        "        \n",
        "\n",
        "\n",
        "        if Mtype in(\"LinearSVC\") and classes != 2: # These classifiers currently only accept binary classification\n",
        "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
        "            return\n",
        "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
        "  \n",
        "\n",
        "\n",
        "            if Mtype in(\"LinearSVC\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
        "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
        "                             .build())\n",
        "\n",
        "            \n",
        "            #Cross Validator requires all of the following parameters:\n",
        "            crossval = CrossValidator(estimator=classifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 + is best practice\n",
        "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "    \n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
        "    \n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "        \n",
        "\n",
        "        if Mtype in(\"LinearSVC\"):\n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            print(\"Intercept: \" + str(BestModel.intercept))\n",
        "            print('\\033[1m' + \"Top 20 Coefficients\"+ '\\033[0m')\n",
        "            print(\"You should compares these relative to eachother\")\n",
        "#             print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
        "            coeff_array = BestModel.coefficients.toArray()\n",
        "            coeff_scores = []\n",
        "            for x in coeff_array:\n",
        "                coeff_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
        "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
        "            # Save the coefficient values and the models\n",
        "            global LSVC_coefficients\n",
        "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
        "            global LSVC_BestModel\n",
        "            LSVC_BestModel = BestModel\n",
        "        \n",
        "   \n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "    \n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        \n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "id": "cV-tNyktsvnz"
    },
    {
      "cell_type": "markdown",
      "id": "jLF_KV7fucSj",
      "metadata": {
        "id": "jLF_KV7fucSj"
      },
      "source": [
        "## test with  or without treat outliers,treat negative values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P6JDAdfXurOO",
      "metadata": {
        "id": "P6JDAdfXurOO"
      },
      "outputs": [],
      "source": [
        "def modeltest(dataframe,classifier,dependent_var:str,input_columns:list,trainsplite:int,testsplite:int,seed:int,folds:int,treat_outliers:bool,treat_neg_values:bool):\n",
        "    \"\"\" return all model accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : dataframe        \n",
        "            \n",
        "            classifier : name of model incude () \n",
        "\n",
        "            dependent_var(str) : estimate the skill of the model on new data \n",
        "\n",
        "            input_columns(list) : input_columns\n",
        "            \n",
        "            trainsplite(int) :   number train splite\n",
        "\n",
        "            testsplite(int) :   number test splite\n",
        "\n",
        "            seed(int) : seed of train and test\n",
        "            \n",
        "            folds(int) : estimate the skill of the model on new data\n",
        "            \n",
        "            treat_outliers(bool) : fix outliers \n",
        "\n",
        "            treat_neg_values(bool) :fix negative value \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return all  accuracy\n",
        "              \n",
        "            \"\"\"\n",
        "    from pyspark.sql.functions import countDistinct\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        return Mtype\n",
        "    Mtype = FindMtype(classifier)\n",
        "  \n",
        "\n",
        "    final_data=MLClassifierDFPrep(dataframe,input_columns,dependent_var,treat_outliers=treat_outliers,treat_neg_values=treat_neg_values)\n",
        "    class_count = dataframe.select(countDistinct(dependent_var)).collect()\n",
        "    classes = class_count[0][0]\n",
        "    classes\n",
        "\n",
        "    train,test = final_data.randomSplit([trainsplite,testsplite],seed)\n",
        "    features = final_data.select(['features']).collect()\n",
        "    \n",
        "    #set up your results table\n",
        "    columns = ['Classifier', 'Result']\n",
        "    vals = [(\"Place Holder\",\"N/A\")]\n",
        "    results = spark.createDataFrame(vals, columns)\n",
        "\n",
        "    if Mtype==\"GBTClassifier\":\n",
        "        new_result = GBTClassifiermodel(features,classes,folds,train,test)\n",
        "    elif Mtype==\"RandomForestClassifier\":\n",
        "        new_result = RandomForestClassifiermodel(features,classes,folds,train,test)\n",
        "    elif Mtype==\"OneVsRest\":\n",
        "        new_result = OneVsRestmodel(features,classes,folds,train,test)\n",
        "    elif Mtype==\"LogisticRegression\":\n",
        "        new_result = LogisticRegressionmodel(features,classes,folds,train,test)\n",
        "    elif Mtype==\"NaiveBayes\":\n",
        "        new_result = NaiveBayesmodel(features,classes,folds,train,test)\n",
        "    elif Mtype==\"DecisionTreeClassifier\":\n",
        "        new_result = DecisionTreeClassifiermodel(features,classes,folds,train,test)\n",
        "    elif Mtype==\"LinearSVC\":\n",
        "        new_result = LinearSVCmodel(features,classes,folds,train,test)\n",
        "    elif Mtype==\"MultilayerPerceptronClassifier\":\n",
        "      new_result = MultilayerPerceptronClassifiermodel(features,classes,folds,train,test) \n",
        "    \n",
        "    results = results.union(new_result)\n",
        "    results = results.where(\"Classifier!='Place Holder'\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wdr16C8dvhv1",
      "metadata": {
        "id": "wdr16C8dvhv1"
      },
      "outputs": [],
      "source": [
        "def  FeatureSelectionrandommodel(dataframe,classifier,input_columns:list,dependent_var:str,trainsplite:int,testsplite:int,seed:int,folds:int,start:int,step:int):  \n",
        "    \"\"\" return all model accuracy  with feature selection\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : dataframe        \n",
        "            \n",
        "            classifier : name of model incude () \n",
        "\n",
        "            dependent_var(str) : estimate the skill of the model on new data \n",
        "\n",
        "            input_columns(list) : input_columns\n",
        "            \n",
        "            trainsplite(int) :   number train splite\n",
        "\n",
        "            testsplite(int) :   number test splite\n",
        "\n",
        "            seed(int) : seed of train and test\n",
        "            \n",
        "            folds(int) : estimate the skill of the model on new data\n",
        "            \n",
        "            treat_outliers(bool) : fix outliers \n",
        "\n",
        "            treat_neg_values(bool) :fix negative value \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return all  accuracy\n",
        "              \n",
        "            \"\"\"\n",
        "    from pyspark.ml.feature import VectorSlicer\n",
        "    from pyspark.ml.feature import ChiSqSelector\n",
        "    from pyspark.sql.functions import countDistinct\n",
        "    from pyspark.ml.linalg import Vectors\n",
        "    import numpy as np\n",
        "    def FindMtype(classifier):\n",
        "          # Intstantiate Model\n",
        "          M = classifier\n",
        "          # Learn what it is\n",
        "          Mtype = type(M).__name__\n",
        "\n",
        "          return Mtype\n",
        "\n",
        "    Mtype = FindMtype(classifier)\n",
        "    test2_data = MLClassifierDFPrep(dataframe,input_columns,dependent_var,treat_outliers=True,treat_neg_values=True)  \n",
        "    class_count = dataframe.select(countDistinct(dependent_var)).collect()\n",
        "    classes = class_count[0][0]\n",
        "    #Select the top n features and view results\n",
        "    maximum = len(input_columns)\n",
        "    for n in np.arange(start, maximum+1 , step).tolist():\n",
        "        print(\"Testing top n = \",n,\" features\")\n",
        "\n",
        "        if Mtype in(\"DecisionTreeClassifier\", \"GBTClassifier\",\"RandomForestClassifier\"):\n",
        "            best_n_features = RF_featureimportances.argsort()[-n:][::-1]\n",
        "            best_n_features= best_n_features.tolist() # convert to a list\n",
        "            vs = VectorSlicer(inputCol=\"features\", outputCol=\"best_features\", indices=best_n_features)\n",
        "            bestFeaturesDf = vs.transform(test2_data)\n",
        "\n",
        "        else:\n",
        "            selector = ChiSqSelector(numTopFeatures=n, featuresCol=\"features\",\n",
        "                                outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
        "            bestFeaturesDf = selector.fit(test2_data).transform(test2_data)\n",
        "            bestFeaturesDf = bestFeaturesDf.select(\"label\",\"selectedFeatures\")\n",
        "            bestFeaturesDf = bestFeaturesDf.withColumnRenamed(\"selectedFeatures\",\"features\")\n",
        "\n",
        "        # Collect features\n",
        "        features = bestFeaturesDf.select(['features']).collect()\n",
        "\n",
        "        # Split\n",
        "        train,test = bestFeaturesDf.randomSplit([trainsplite,testsplite],seed)\n",
        "\n",
        "        # Specify folds\n",
        "\n",
        "\n",
        "        #set up your results table\n",
        "        columns = ['Classifier', 'Result']\n",
        "        vals = [(\"Place Holder\",\"N/A\")]\n",
        "        results = spark.createDataFrame(vals, columns)\n",
        "\n",
        "\n",
        "        if Mtype==\"GBTClassifier\":\n",
        "            new_result = GBTClassifiermodel(features,classes,folds,train,test)\n",
        "        elif Mtype==\"RandomForestClassifier\":\n",
        "            new_result = RandomForestClassifiermodel(features,classes,folds,train,test)\n",
        "        elif Mtype==\"OneVsRest\":\n",
        "            new_result = OneVsRestmodel(features,classes,folds,train,test)\n",
        "        elif Mtype==\"LogisticRegression\":\n",
        "            new_result = LogisticRegressionmodel(features,classes,folds,train,test)\n",
        "        elif Mtype==\"NaiveBayes\":\n",
        "            new_result = NaiveBayesmodel(features,classes,folds,train,test)\n",
        "        elif Mtype==\"DecisionTreeClassifier\":\n",
        "            new_result = DecisionTreeClassifiermodel(features,classes,folds,train,test)\n",
        "        elif Mtype==\"LinearSVC\":\n",
        "            new_result = LinearSVCmodel(features,classes,folds,train,test)\n",
        "        elif Mtype==\"MultilayerPerceptronClassifier\":\n",
        "          new_result = MultilayerPerceptronClassifiermodel(features,classes,folds,train,test) \n",
        "\n",
        "        results = results.union(new_result)\n",
        "        results = results.where(\"Classifier!='Place Holder'\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##text minning"
      ],
      "metadata": {
        "id": "-3H94IVdjGl6"
      },
      "id": "-3H94IVdjGl6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### return dataframe pipeline with  regex_tokenizer,remover,indexer satges.\n"
      ],
      "metadata": {
        "id": "MaNtCXY_jK9m"
      },
      "id": "MaNtCXY_jK9m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0655cd",
      "metadata": {
        "id": "ce0655cd"
      },
      "outputs": [],
      "source": [
        "def data_prep_pl(dataframe,inputCols:list,inputColtarget:str):\n",
        "  \"\"\" return dataframe pipeline with  regex_tokenizer,remover,indexer satges.\n",
        "             \n",
        "      Parameters:\n",
        "      -------------- \n",
        "                   \n",
        "      dataframe(:class:`DataFrame) : select dataframe                       \n",
        "      \n",
        "      inputCols(list):  input columns of datafram\n",
        "    \n",
        "      inputColtarget(str):  ccolumn target of dataframe\n",
        "       \n",
        "\n",
        "  \n",
        "\n",
        "            \n",
        "      Returns:\n",
        "      -------------\n",
        "      feature_data (:class:`DataFrame) : new dataframe with trim a column\n",
        "\n",
        "\n",
        "  \n",
        "            \n",
        "  \"\"\" \n",
        "  from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
        "  from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
        "  from pyspark.sql.types import StringType,IntegerType\n",
        "  from pyspark.ml import Pipeline \n",
        "  feature_data=dataframe\n",
        "  indexer = StringIndexer(inputCol=inputColtarget, outputCol=\"label\")\n",
        "  \n",
        "  for count, inputCol in enumerate(inputCols):\n",
        "        inputCol=inputCol\n",
        "        outputColRE=\"words\"+str(count+1)\n",
        "        filteredcolumn=\"filtered\"+str(count+1)\n",
        "        # Tokenize\n",
        "        regex_tokenizer = RegexTokenizer(inputCol=inputCol, outputCol=outputColRE, pattern=\"\\\\W\")\n",
        "\n",
        "        remover = StopWordsRemover(inputCol=regex_tokenizer.getOutputCol(), outputCol=filteredcolumn)\n",
        "\n",
        "              # Zero Index Label Column\n",
        "        \n",
        "  \n",
        "        # Create the Pipeline\n",
        "        pipeline = Pipeline(stages=[regex_tokenizer,remover])\n",
        "        data_prep_pl = pipeline.fit(feature_data)\n",
        "              \n",
        "\n",
        "        feature_data = data_prep_pl.transform(feature_data)\n",
        "  pipeline = Pipeline(stages=[indexer])\n",
        "  data_prep_pl = pipeline.fit(feature_data)\n",
        "              \n",
        "\n",
        "  feature_data = data_prep_pl.transform(feature_data)\n",
        "  return feature_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hashingTFIDF(feature_data,inputCols:list,list,numFeatures:int)->tuple: \n",
        "  \"\"\" return HTFfeaturizedData,TFIDFfeaturizedData\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            feature_data(:class:`DataFrame) : select data_prep_pl           \n",
        "            \n",
        "                       \n",
        "            inputCols(list):  filtered columns  for next uses.\n",
        "\n",
        "\n",
        "            numFeatures(str): number of feature \n",
        "\n",
        "         \n",
        "\n",
        "  \n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            (tuple) : HTFfeaturizedData,TFIDFfeaturizedData\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "  \"\"\" \n",
        "  HTFfeaturizedData=feature_data\n",
        "  for count, inputCol in enumerate(inputCols):\n",
        "          inputCol=\"filtered\"+str(count+1)\n",
        "          outputCol=\"rawfeatures\"+str(count+1)\n",
        " \n",
        "          hashingTF = HashingTF(inputCol=inputCol, outputCol=outputCol, numFeatures=numFeatures)\n",
        "          HTFfeaturizedData = hashingTF.transform(HTFfeaturizedData)\n",
        "          HTFfeaturizedData.name = 'HTFfeaturizedData' #We will use later for printing\n",
        "  TFIDFfeaturizedData=HTFfeaturizedData\n",
        "  for count, inputCol in enumerate(inputCols):\n",
        "      inputCol=\"rawfeatures\"+str(count+1)\n",
        "      outputCol=\"features\"+str(count+1)\n",
        "      idf = IDF(inputCol=hashingoutputCol, outputCol=outputCol)\n",
        "      idfModel = idf.fit(HTFfeaturizedData)\n",
        "      TFIDFfeaturizedData = idfModel.transform(TFIDFfeaturizedData)\n",
        "      TFIDFfeaturizedData.name = 'TFIDFfeaturizedData'\n",
        "  for count, inputCols in enumerate(inputCols): \n",
        "      inputCol=\"rawfeatures\"+str(count+1)\n",
        "      outputCol=\"features\"+str(count+1)  \n",
        "      HTFfeaturizedData = HTFfeaturizedData.withColumnRenamed(inputCol,outputCol)\n",
        "   \n",
        "  return HTFfeaturizedData,TFIDFfeaturizedData"
      ],
      "metadata": {
        "id": "t6zrWw1WjYLP"
      },
      "id": "t6zrWw1WjYLP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def words2vec(feature_data,inputCol:list,inputColtarget:str): \n",
        "    \"\"\" return W2VfeaturizedData\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            feature_data(:class:`DataFrame) : select dataframe from data_prep_pl                      \n",
        "                       \n",
        "            inputCol(list):  input coloumns.\n",
        "\n",
        "            inputColtarget(str):  ccolumn target of dataframe\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            W2VfeaturizedData(:class:`DataFrame) : return word to vector dataframe\n",
        "\n",
        "      \n",
        "    \"\"\" \n",
        "    from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
        "    from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
        "    from pyspark.sql.types import StringType,IntegerType\n",
        "\n",
        "    W2VfeaturizedData=feature_data\n",
        "    for count, inputCol in enumerate(inputCols):\n",
        "          inputCol=\"filtered\"+str(count+1)\n",
        "          outputCol=\"features\"+str(count+1)\n",
        "          word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=inputCol, outputCol=outputCol)\n",
        "          model = word2Vec.fit(W2VfeaturizedData)\n",
        "\n",
        "          W2VfeaturizedData = model.transform(W2VfeaturizedData)\n",
        "          \n",
        "    scaled_data=W2VfeaturizedData\n",
        "    for count, inputCol in enumerate(inputCols):\n",
        "          inputCol=\"features\"+str(count+1)\n",
        "          outputCol=\"rawfeatures\"+str(count+1)\n",
        "          # W2Vec Dataframes typically has negative values so we will correct for that here so that we can use the Naive Bayes classifier\n",
        "          scaler = MinMaxScaler(inputCol=inputCol, outputCol=outputCol)\n",
        "\n",
        "          # Compute summary statistics and generate MinMaxScalerModel\n",
        "          scalerModel = scaler.fit(scaled_data)\n",
        "\n",
        "          # rescale each feature to range [min, max].\n",
        "          scaled_data = scalerModel.transform(scaled_data)\n",
        "    scaled_data = scaled_data.drop(\"features1\",\"features2\")\n",
        "    for count, inputCol in enumerate(inputCols):\n",
        "        inputCol=\"rawfeatures\"+str(count+1)\n",
        "        outputCol=\"features\"+str(count+1)    \n",
        "        scaled_data = scaled_data.withColumnRenamed(inputCol,outputCol)\n",
        "\n",
        "    W2VfeaturizedData.name = 'W2VfeaturizedData'\n",
        "    W2VfeaturizedData=scaled_data\n",
        "    return W2VfeaturizedData\n"
      ],
      "metadata": {
        "id": "0Q-Mod9bmUYO"
      },
      "id": "0Q-Mod9bmUYO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def words2vec(feature_data,vectorSize:int,minCount:int,filteredcolumns:list,featureCols:list,middleputCols:list,inputColtarget:str): \n",
        "    \"\"\" return W2VfeaturizedData\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            feature_data(:class:`DataFrame) : select dataframe from data_prep_pl                      \n",
        "                       \n",
        "            filteredcolumns(str):  filtered columns  for next uses.\n",
        "\n",
        "            featureCols(str):  main features for next step \n",
        "\n",
        "            middleputCols(str):  middle column for renaming usage\n",
        "\n",
        "            inputColtarget(str):  ccolumn target of dataframe\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "            \n",
        "        Returns:\n",
        "        --------------\n",
        "            (tuple) : HTFfeaturizedData,TFIDFfeaturizedData\n",
        "\n",
        "        Examlpe:\n",
        "        --------------\n",
        "            middleputCols=[\"rawfeatures1\",\"rawfeatures2\"]\n",
        "            featureCols=[\"features1\",\"features2\"]\n",
        "\n",
        "            \n",
        "    \"\"\" \n",
        "    \n",
        "    W2VfeaturizedData=feature_data\n",
        "    for count, filteredcolumn in enumerate(filteredcolumns):\n",
        "          inputCol=filteredcolumn\n",
        "          outputCol=featureCols[count]  \n",
        "          word2Vec = Word2Vec(vectorSize=vectorSize, minCount=minCount, inputCol=inputCol, outputCol=outputCol)\n",
        "          model = word2Vec.fit(W2VfeaturizedData)\n",
        "\n",
        "          W2VfeaturizedData = model.transform(W2VfeaturizedData)\n",
        "          \n",
        "    scaled_data=W2VfeaturizedData\n",
        "    for count, featureCol in enumerate(featureCols):\n",
        "          inputCol=featureCol\n",
        "          outputCol=middleputCols[count]\n",
        "          # W2Vec Dataframes typically has negative values so we will correct for that here so that we can use the Naive Bayes classifier\n",
        "          scaler = MinMaxScaler(inputCol=inputCol, outputCol=outputCol)\n",
        "\n",
        "          # Compute summary statistics and generate MinMaxScalerModel\n",
        "          scalerModel = scaler.fit(scaled_data)\n",
        "\n",
        "          # rescale each feature to range [min, max].\n",
        "          scaled_data = scalerModel.transform(scaled_data)\n",
        "    scaled_data = scaled_data.drop(\"features1\",\"features2\")\n",
        "    for count, middleputCol in enumerate(middleputCols):\n",
        "        inputCol=middleputCol\n",
        "        outputCol=featureCols[count]    \n",
        "        scaled_data = scaled_data.withColumnRenamed(inputCol,outputCol)\n",
        "\n",
        "    W2VfeaturizedData.name = 'W2VfeaturizedData'\n",
        "    W2VfeaturizedData=scaled_data\n",
        "    return W2VfeaturizedData\n"
      ],
      "metadata": {
        "id": "g_79dwYpmnvT"
      },
      "id": "g_79dwYpmnvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def featureDF(dataframe,classifier,trainsplite:int,testsplite:int,seed:int,folds:int,featureDFs:tuple):\n",
        "    \"\"\" return all model accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            dataframe(:class:`DataFrame) : dataframe        \n",
        "            \n",
        "            classifier : name of model incude () \n",
        "\n",
        "            \n",
        "            trainsplite(int) :   number train splite\n",
        "\n",
        "            testsplite(int) :   number test splite\n",
        "\n",
        "            seed(int) : seed of train and test\n",
        "            \n",
        "            folds(int) : estimate the skill of the model on new data\n",
        "            \n",
        "            featureDFs(tuple) : HTFfeaturizedData,TFIDFfeaturizedData,W2VfeaturizedData\n",
        "\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return all  accuracy\n",
        "              \n",
        "    \"\"\"\n",
        "    from pyspark.sql.functions import countDistinct\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        return Mtype\n",
        "    Mtype = FindMtype(classifier)\n",
        "    for featureDF in featureDFs:\n",
        "\n",
        "      print('\\033[1m' + featureDF.name,\" Results:\"+ '\\033[0m')\n",
        "      train, test = featureDF.randomSplit([trainsplite, testsplite],seed = seed)\n",
        "      features = featureDF.select(['features']).collect()\n",
        "      # Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
        "      class_count = featureDF.select(countDistinct(\"label\")).collect()\n",
        "      classes = class_count[0][0]\n",
        "\n",
        "      #set up your results table\n",
        "      columns = ['Classifier', 'Result']\n",
        "      vals = [(\"Place Holder\",\"N/A\")]\n",
        "      results = spark.createDataFrame(vals, columns)\n",
        "\n",
        "\n",
        "      if Mtype==\"GBTClassifier\":\n",
        "          new_result = GBTClassifiermodel(features,classes,folds,train,test)\n",
        "      elif Mtype==\"RandomForestClassifier\":\n",
        "          new_result = RandomForestClassifiermodel(features,classes,folds,train,test)\n",
        "      elif Mtype==\"OneVsRest\":\n",
        "          new_result = OneVsRestmodel(features,classes,folds,train,test)\n",
        "      elif Mtype==\"LogisticRegression\":\n",
        "          new_result = LogisticRegressionmodel(features,classes,folds,train,test)\n",
        "      elif Mtype==\"NaiveBayes\":\n",
        "          new_result = NaiveBayesmodel(features,classes,folds,train,test)\n",
        "      elif Mtype==\"DecisionTreeClassifier\":\n",
        "          new_result = DecisionTreeClassifiermodel(features,classes,folds,train,test)\n",
        "      elif Mtype==\"LinearSVC\":\n",
        "          new_result = LinearSVCmodel(features,classes,folds,train,test)\n",
        "      elif Mtype==\"MultilayerPerceptronClassifier\":\n",
        "        new_result = MultilayerPerceptronClassifiermodel(features,classes,folds,train,test) \n",
        "    "
      ],
      "metadata": {
        "id": "GdynECl2nDgB"
      },
      "id": "GdynECl2nDgB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## regression model"
      ],
      "metadata": {
        "id": "pjXbtkfs3DbU"
      },
      "id": "pjXbtkfs3DbU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prepration data"
      ],
      "metadata": {
        "id": "vjqVwmOQ3lLU"
      },
      "id": "vjqVwmOQ3lLU"
    },
    {
      "cell_type": "code",
      "source": [
        "def MLRegressDFPrep(df,input_columns,dependent_var,treat_outliers=True):\n",
        "    \"\"\" return a dataframe with data prepration ( checking skewness and outliers)\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "                   \n",
        "            df(:class:`DataFrame) : select dataframe        \n",
        "            \n",
        "            inputCols(str) : name of input columns \n",
        "\n",
        "            dependent_var(str) : name of dependent columns \n",
        "\n",
        "            treat_outliers(bool) : modify dataframe for outliers\n",
        "\n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            final_data (:class:`DataFrame) : the  dataframe  with data prepration \n",
        "              \n",
        "            \n",
        "            \"\"\"  \n",
        "  \n",
        "    renamed = df.withColumnRenamed(dependent_var,'label')\n",
        "    \n",
        "    # Make sure dependent variable is numeric and change if it's not\n",
        "    if str(renamed.schema['label'].dataType) != 'IntegerType':\n",
        "        renamed = renamed.withColumn(\"label\", renamed[\"label\"].cast(FloatType()))\n",
        "    \n",
        "   # Convert all string type data in the input column list to numeric\n",
        "    # Otherwise the Algorithm will not be able to process it\n",
        "    numeric_inputs = []\n",
        "    string_inputs = []\n",
        "    for column in input_columns:\n",
        "        if str(renamed.schema[column].dataType) == 'StringType':\n",
        "            new_col_name = column+\"_num\"\n",
        "            string_inputs.append(new_col_name)\n",
        "        else:\n",
        "            numeric_inputs.append(column)\n",
        "            indexed = renamed\n",
        "            \n",
        "    if len(string_inputs) != 0: # If the datafraem contains string types\n",
        "        for column in input_columns:\n",
        "            if str(renamed.schema[column].dataType) == 'StringType':\n",
        "                indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
        "                indexed = indexer.fit(renamed).transform(renamed)\n",
        "    else:\n",
        "        indexed = renamed\n",
        "        \n",
        "            \n",
        "    if treat_outliers == True:\n",
        "        print(\"We are correcting for non normality now!\")\n",
        "        # empty dictionary d\n",
        "        d = {}\n",
        "        # Create a dictionary of quantiles\n",
        "        for col in numeric_inputs: \n",
        "            d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
        "        #Now fill in the values\n",
        "        for col in numeric_inputs:\n",
        "            skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
        "            skew = skew[0][0]\n",
        "            # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
        "            if skew > 1:\n",
        "                indexed = indexed.withColumn(col, \\\n",
        "                log(when(df[col] < d[col][0],d[col][0])\\\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "                .otherwise(indexed[col] ) +1).alias(col))\n",
        "                print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
        "            elif skew < -1:\n",
        "                indexed = indexed.withColumn(col, \\\n",
        "                exp(when(df[col] < d[col][0],d[col][0])\\\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "                .otherwise(indexed[col] )).alias(col))\n",
        "                print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
        "\n",
        "            \n",
        "    # Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
        "    # Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
        "    minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) # Calculate the mins for all columns in the df\n",
        "    min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) # Create an array for all mins and select only the input cols\n",
        "    df_minimum = min_array.select(array_min(min_array.mins)).collect() # Collect golobal min as Python object\n",
        "    df_minimum = df_minimum[0][0] # Slice to get the number itself\n",
        "\n",
        "    features_list = numeric_inputs + string_inputs\n",
        "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
        "    output = assembler.transform(indexed).select('features','label')\n",
        "\n",
        "#     final_data = output.select('features','label') #drop everything else\n",
        "    \n",
        "    # Now check for negative values and ask user if they want to correct that? \n",
        "    if df_minimum < 0:\n",
        "        print(\" \")\n",
        "        print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
        "        print(\" \")\n",
        "    \n",
        "    if treat_neg_values == True:\n",
        "        print(\"You have opted to correct that by rescaling all your features to a range of 0 to 1\")\n",
        "        print(\" \")\n",
        "        print(\"We are rescaling you dataframe....\")\n",
        "        scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "        # Compute summary statistics and generate MinMaxScalerModel\n",
        "        global scalerModel\n",
        "        scalerModel = scaler.fit(output)\n",
        "\n",
        "        # rescale each feature to range [min, max].\n",
        "        scaled_data = scalerModel.transform(output)\n",
        "        final_data = scaled_data.select('label','scaledFeatures')\n",
        "        final_data = final_data.withColumnRenamed('scaledFeatures','features')\n",
        "        print(\"Done!\")\n",
        "\n",
        "    else:\n",
        "        print(\"You have opted not to correct that therefore you will not be able to use to Naive Bayes classifier\")\n",
        "        print(\"We will return the dataframe unscaled.\")\n",
        "        final_data = output\n",
        "    \n",
        "    return final_data"
      ],
      "metadata": {
        "id": "vzs8vz0M3CYQ"
      },
      "id": "vzs8vz0M3CYQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Regression model"
      ],
      "metadata": {
        "id": "BV5ADF5l5eig"
      },
      "id": "BV5ADF5l5eig"
    },
    {
      "cell_type": "code",
      "source": [
        "def LinearRegressionmodel(dataframe,input_columns:list,dependent_var:str,trainesplite:int,testsplite:int,treat_outliers=True):\n",
        "    \"\"\" return Linear Regression accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "            dataframe  (:class:`DataFrame):main dataframe\n",
        "\n",
        "            dependent_var(str) : dependent varvariable column  \n",
        "\n",
        "            input_columns(list)  : name of input columns \n",
        "            \n",
        "            train(int) : train splite  \n",
        "            \n",
        "            test(int) :   test splite \n",
        "\n",
        "            treat_outliers(bool) : treating outliers\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Linear Regression accuracy \n",
        "              \n",
        "            \"\"\"\n",
        "    final_data=MLRegressDFPrep(dataframe,input_columns,dependent_var,treat_outliers)\n",
        "    train,test = final_data.randomSplit([trainesplite,testsplite])\n",
        "    regressor=LinearRegression()\n",
        "    def FindMtype(regressor):\n",
        "        # Intstantiate Model\n",
        "        M = regressor\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(regressor)\n",
        "#     print('\\033[1m' + Mtype + ':' + '\\033[0m')\n",
        "\n",
        "\n",
        "    if Mtype == \"LinearRegression\":\n",
        "        \n",
        "           Mtype = FindMtype(regressor)\n",
        "#     print('\\033[1m' + Mtype + ':' + '\\033[0m')\n",
        "\n",
        "\n",
        "    if Mtype == \"LinearRegression\":\n",
        "        \n",
        "        #first without cross val\n",
        "        fitModel = regressor.fit(train)\n",
        "\n",
        "        # Load the Summary\n",
        "        trainingSummary = fitModel.summary\n",
        "        \n",
        "        # Print the coefficients and intercept for linear regression\n",
        "        print('\\033[1m' + \"Linear Regression Model Summary without cross validation:\"+ '\\033[0m')\n",
        "        print(\" \")\n",
        "        print(\"Coefficients: %s\" % str(fitModel.coefficients))\n",
        "        print(\"Intercept: %s\" % str(fitModel.intercept))\n",
        "        print(\"\")\n",
        "\n",
        "        # Summarize the model over the training set and print out some metrics\n",
        "        print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
        "        print(\"objectiveHistory: (scaled loss + regularization) at each iteration \\n %s\" % str(trainingSummary.objectiveHistory))\n",
        "        print(\"\")\n",
        "        \n",
        "        # Print the Errors\n",
        "        print(\"Training RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "        print(\"Training r2: %f\" % trainingSummary.r2)\n",
        "        print(\"\")\n",
        "        \n",
        "\n",
        "        # Now load the test results\n",
        "        test_results = fitModel.evaluate(test)\n",
        "\n",
        "        # And print them\n",
        "        print(\"Test RMSE: {}\".format(test_results.rootMeanSquaredError))\n",
        "        print(\"Test r2: {}\".format(test_results.r2))\n",
        "        print(\"\")\n",
        "        \n",
        "        #Now train with cross val\n",
        "        paramGrid = (ParamGridBuilder() \\\n",
        "#              .addGrid(regressor.maxIter, [10, 15]) \\\n",
        "             .addGrid(regressor.regParam, [0.1, 0.01]) \\\n",
        "             .build())\n",
        "        \n",
        "        #Evaluator\n",
        "        revaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "        \n",
        "        #Cross Validator requires all of the following parameters:\n",
        "        crossval = CrossValidator(estimator=regressor,\n",
        "                                  estimatorParamMaps=paramGrid,\n",
        "                                  evaluator=revaluator,\n",
        "                                  numFolds=2) # 3 is best practice\n",
        "        \n",
        "        print('\\033[1m' + \"Linear Regression Model Summary WITH cross validation:\"+ '\\033[0m')\n",
        "        print(\" \")\n",
        "        # Run cross validations\n",
        "        fitModel = crossval.fit(train)\n",
        "        \n",
        "        # Get Model Summary Statistics\n",
        "        ModelSummary = fitModel.bestModel.summary\n",
        "        print(\"Coefficient Standard Errors: \" + str(ModelSummary.coefficientStandardErrors))\n",
        "        print(\" \")\n",
        "        print(\"P Values: \" + str(ModelSummary.pValues)) # Last element is the intercept\n",
        "        print(\" \")\n",
        "        \n",
        "        global LR_Pvalues\n",
        "        LR_Pvalues = ModelSummary.pValues\n",
        "        \n",
        "        #save model\n",
        "        global LR_BestModel \n",
        "        LR_BestModel = fitModel.bestModel\n",
        "        \n",
        "        # Use test set here so we can measure the accuracy of our model on new data\n",
        "        ModelPredictions = fitModel.transform(test)\n",
        "        \n",
        "        # cvModel uses the best model found from the Cross Validation\n",
        "        # Evaluate best model\n",
        "        test_results = revaluator.evaluate(ModelPredictions)\n",
        "        print('RMSE:', test_results)\n",
        "    \n",
        "        # Set the column names to match the external results dataframe that we will join with later:\n",
        "        columns = ['Regressor', 'Result']\n",
        "        \n",
        "        # Format results and return\n",
        "        rmse_str = [str(test_results)] #make this a string and convert to a list\n",
        "        Mtype = [Mtype] #make this a string\n",
        "        result = spark.createDataFrame(zip(Mtype,rmse_str), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        return result"
      ],
      "metadata": {
        "id": "3v16bs6vpF_0"
      },
      "id": "3v16bs6vpF_0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Random Forest Regressor"
      ],
      "metadata": {
        "id": "1947Bf0TCTnc"
      },
      "id": "1947Bf0TCTnc"
    },
    {
      "cell_type": "code",
      "source": [
        "def RandomForestRegressormodel(dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers=True):\n",
        "    \"\"\" return Random Forest Regressor accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "            dataframe  (:class:`DataFrame):main dataframe\n",
        "\n",
        "            dependent_var(str) : dependent varvariable column  \n",
        "\n",
        "            input_columns(list)  : name of input columns \n",
        "            \n",
        "            train(int) : train splite  \n",
        "            \n",
        "            test(int) :   test splite \n",
        "\n",
        "            treat_outliers(bool) : treating outliers\n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Random Forest Regressor accuracy \n",
        "              \n",
        "            \"\"\"\n",
        "    final_data=MLRegressDFPrep(dataframe,input_columns,dependent_var,treat_outliers)\n",
        "    train,test = final_data.randomSplit([trainesplite,testsplite])\n",
        "    regressor=RandomForestRegressor()\n",
        "    def FindMtype(regressor):\n",
        "        # Intstantiate Model\n",
        "        M = regressor\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(regressor)\n",
        "#     print('\\033[1m' + Mtype + ':' + '\\033[0m')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if not Mtype == \"LinearRegression\":\n",
        "\n",
        "        # Add parameters of your choice here:\n",
        "        if Mtype in(\"RandomForestRegressor\"):\n",
        "            paramGrid = (ParamGridBuilder() \\\n",
        "#                            .addGrid(regressor.maxDepth, [2, 5, 10])\n",
        "#                            .addGrid(regressor.maxBins, [5, 10, 20])\n",
        "                           .addGrid(regressor.numTrees, [5, 20])\n",
        "                         .build())\n",
        "\n",
        "        # Add parameters of your choice here:\n",
        "       \n",
        "\n",
        "        #Cross Validator requires all of the following parameters:\n",
        "        crossval = CrossValidator(estimator=regressor,\n",
        "                                  estimatorParamMaps=paramGrid,\n",
        "                                  evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
        "                                  numFolds=2) # 3 is best practice\n",
        "        # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "        fitModel = crossval.fit(train)\n",
        "        \n",
        "        # Get Best Model\n",
        "        BestModel = fitModel.bestModel\n",
        "\n",
        "        # FEATURE IMPORTANCES\n",
        "        # Estimate of the importance of each feature.\n",
        "        # Each feature’s importance is the average of its importance across all trees \n",
        "        # in the ensemble The importance vector is normalized to sum to 1. \n",
        "        print(\" \")\n",
        "        print('\\033[1m' + Mtype,\" Feature Importances\"+ '\\033[0m')\n",
        "        print(\"(Scores add up to 1)\")\n",
        "        print(\"Lowest score is the least important\")\n",
        "        print(\" \")\n",
        "        featureImportances = BestModel.featureImportances.toArray()\n",
        "        # Convert from numpy array to list\n",
        "        imp_scores = []\n",
        "        for x in featureImportances:\n",
        "            imp_scores.append(float(x))\n",
        "        # Then zip with input_columns list and create a df\n",
        "        result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "        print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
        "        \n",
        "        #Create Global Variables for feature importances and models\n",
        "\n",
        "        if Mtype in(\"RandomForestRegressor\"):\n",
        "            global RF_featureImportances\n",
        "            RF_featureImportances = BestModel.featureImportances.toArray()\n",
        "            global RF_BestModel \n",
        "            RF_BestModel = fitModel.bestModel\n",
        "                    \n",
        "        # Set the column names to match the external results dataframe that we will join with later:\n",
        "        columns = ['Regressor', 'Result']\n",
        "        \n",
        "        # Make predictions.\n",
        "        predictions = fitModel.transform(test)\n",
        "        # Select (prediction, true label) and compute test error\n",
        "        evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "        rmse = evaluator.evaluate(predictions)\n",
        "        rmse_str = [str(rmse)] #make this a string and convert to a list\n",
        "        Mtype = [Mtype] #make this a string\n",
        "        result = spark.createDataFrame(zip(Mtype,rmse_str), schema=columns)\n",
        "        # Clean up the Result column and output\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        return result"
      ],
      "metadata": {
        "id": "JTUWM4IYyI8k"
      },
      "id": "JTUWM4IYyI8k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GBT Regressor "
      ],
      "metadata": {
        "id": "9BKXlOENCZO_"
      },
      "id": "9BKXlOENCZO_"
    },
    {
      "cell_type": "code",
      "source": [
        "def GBTRegressormodel(dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers):\n",
        "    \"\"\" return GBT Regressor accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "            dataframe  (:class:`DataFrame):main dataframe\n",
        "\n",
        "            dependent_var(str) : dependent varvariable column  \n",
        "\n",
        "            input_columns(list)  : name of input columns \n",
        "            \n",
        "            train(int) : train splite  \n",
        "            \n",
        "            test(int) :   test splite \n",
        "\n",
        "            treat_outliers(bool) : treating outliers\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return GBT Regressor accuracy \n",
        "              \n",
        "            \"\"\"\n",
        "    final_data=MLRegressDFPrep(dataframe,input_columns,dependent_var,treat_outliers)\n",
        "    train,test = final_data.randomSplit([trainesplite,testsplite])\n",
        "    regressor=GBTRegressor()\n",
        "    def FindMtype(regressor):\n",
        "        # Intstantiate Model\n",
        "        M = regressor\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(regressor)\n",
        "#     print('\\033[1m' + Mtype + ':' + '\\033[0m')\n",
        "\n",
        "\n",
        "    if not Mtype == \"LinearRegression\":\n",
        "        \n",
        "       \n",
        "        # Add parameters of your choice here:\n",
        "        if Mtype in(\"GBTRegressor\"):\n",
        "            paramGrid = (ParamGridBuilder() \\\n",
        "#                          .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "                         .addGrid(regressor.maxBins, [10, 20]) \\\n",
        "                         .addGrid(regressor.maxIter, [10, 15])\n",
        "                         .build())\n",
        "\n",
        "  \n",
        "\n",
        "        #Cross Validator requires all of the following parameters:\n",
        "        crossval = CrossValidator(estimator=regressor,\n",
        "                                  estimatorParamMaps=paramGrid,\n",
        "                                  evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
        "                                  numFolds=2) # 3 is best practice\n",
        "        # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "        fitModel = crossval.fit(train)\n",
        "        \n",
        "        # Get Best Model\n",
        "        BestModel = fitModel.bestModel\n",
        "\n",
        "        # FEATURE IMPORTANCES\n",
        "        # Estimate of the importance of each feature.\n",
        "        # Each feature’s importance is the average of its importance across all trees \n",
        "        # in the ensemble The importance vector is normalized to sum to 1. \n",
        "        print(\" \")\n",
        "        print('\\033[1m' + Mtype,\" Feature Importances\"+ '\\033[0m')\n",
        "        print(\"(Scores add up to 1)\")\n",
        "        print(\"Lowest score is the least important\")\n",
        "        print(\" \")\n",
        "        featureImportances = BestModel.featureImportances.toArray()\n",
        "        # Convert from numpy array to list\n",
        "        imp_scores = []\n",
        "        for x in featureImportances:\n",
        "            imp_scores.append(float(x))\n",
        "        # Then zip with input_columns list and create a df\n",
        "        result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "        print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
        "        \n",
        "        #Create Global Variables for feature importances and models\n",
        "\n",
        "        if Mtype in(\"GBTRegressor\"):\n",
        "            global GBT_featureImportances\n",
        "            GBT_featureImportances = BestModel.featureImportances.toArray()\n",
        "            global GBT_BestModel \n",
        "            GBT_BestModel = fitModel.bestModel\n",
        "\n",
        "                    \n",
        "        # Set the column names to match the external results dataframe that we will join with later:\n",
        "        columns = ['Regressor', 'Result']\n",
        "        \n",
        "        # Make predictions.\n",
        "        predictions = fitModel.transform(test)\n",
        "        # Select (prediction, true label) and compute test error\n",
        "        evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "        rmse = evaluator.evaluate(predictions)\n",
        "        rmse_str = [str(rmse)] #make this a string and convert to a list\n",
        "        Mtype = [Mtype] #make this a string\n",
        "        result = spark.createDataFrame(zip(Mtype,rmse_str), schema=columns)\n",
        "        # Clean up the Result column and output\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        return result"
      ],
      "metadata": {
        "id": "zzATUdgd26Wp"
      },
      "id": "zzATUdgd26Wp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Regressor"
      ],
      "metadata": {
        "id": "HvbkVNsPCoYz"
      },
      "id": "HvbkVNsPCoYz"
    },
    {
      "cell_type": "code",
      "source": [
        "def DecisionTreeRegressormodel(dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers=True):\n",
        "    \"\"\" return  Decision Tree Regressor accuracy \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "            dataframe  (:class:`DataFrame):main dataframe\n",
        "\n",
        "            dependent_var(str) : dependent varvariable column  \n",
        "\n",
        "            input_columns(list)  : name of input columns \n",
        "            \n",
        "            train(int) : train splite  \n",
        "            \n",
        "            test(int) :   test splite \n",
        "\n",
        "            treat_outliers(bool) : treating outliers\n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return Decision Tree Regressor accuracy  \n",
        "              \n",
        "            \"\"\"\n",
        "    final_data=MLRegressDFPrep(dataframe,input_columns,dependent_var,treat_outliers)\n",
        "    train,test = final_data.randomSplit([trainesplite,testsplite])\n",
        "    regressor=DecisionTreeRegressor()\n",
        "    def FindMtype(regressor):\n",
        "        # Intstantiate Model\n",
        "        M = regressor\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "    Mtype = FindMtype(regressor)\n",
        "#     print('\\033[1m' + Mtype + ':' + '\\033[0m')\n",
        "\n",
        "\n",
        "    if not Mtype == \"LinearRegression\":\n",
        "        \n",
        "       \n",
        "\n",
        "        # Add parameters of your choice here:\n",
        "        if Mtype in(\"DecisionTreeRegressor\"):\n",
        "            paramGrid = (ParamGridBuilder() \\\n",
        "#                          .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "                         .addGrid(regressor.maxBins, [10, 20, 40]) \\\n",
        "                         .build())\n",
        "\n",
        "        #Cross Validator requires all of the following parameters:\n",
        "        crossval = CrossValidator(estimator=regressor,\n",
        "                                  estimatorParamMaps=paramGrid,\n",
        "                                  evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
        "                                  numFolds=2) # 3 is best practice\n",
        "        # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "        fitModel = crossval.fit(train)\n",
        "        \n",
        "        # Get Best Model\n",
        "        BestModel = fitModel.bestModel\n",
        "\n",
        "        # FEATURE IMPORTANCES\n",
        "        # Estimate of the importance of each feature.\n",
        "        # Each feature’s importance is the average of its importance across all trees \n",
        "        # in the ensemble The importance vector is normalized to sum to 1. \n",
        "        print(\" \")\n",
        "        print('\\033[1m' + Mtype,\" Feature Importances\"+ '\\033[0m')\n",
        "        print(\"(Scores add up to 1)\")\n",
        "        print(\"Lowest score is the least important\")\n",
        "        print(\" \")\n",
        "        featureImportances = BestModel.featureImportances.toArray()\n",
        "        # Convert from numpy array to list\n",
        "        imp_scores = []\n",
        "        for x in featureImportances:\n",
        "            imp_scores.append(float(x))\n",
        "        # Then zip with input_columns list and create a df\n",
        "        result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "        print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
        "        \n",
        "        #Create Global Variables for feature importances and models\n",
        "        if Mtype in(\"DecisionTreeRegressor\"):\n",
        "            global DT_featureImportances\n",
        "            DT_featureImportances = BestModel.featureImportances.toArray()\n",
        "            global DT_BestModel \n",
        "            DT_BestModel = fitModel.bestModel\n",
        "\n",
        "                    \n",
        "        # Set the column names to match the external results dataframe that we will join with later:\n",
        "        columns = ['Regressor', 'Result']\n",
        "        \n",
        "        # Make predictions.\n",
        "        predictions = fitModel.transform(test)\n",
        "        # Select (prediction, true label) and compute test error\n",
        "        evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "        rmse = evaluator.evaluate(predictions)\n",
        "        rmse_str = [str(rmse)] #make this a string and convert to a list\n",
        "        Mtype = [Mtype] #make this a string\n",
        "        result = spark.createDataFrame(zip(Mtype,rmse_str), schema=columns)\n",
        "        # Clean up the Result column and output\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "        return result"
      ],
      "metadata": {
        "id": "1BNpdgPF71PD"
      },
      "id": "1BNpdgPF71PD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run  all regression models"
      ],
      "metadata": {
        "id": "tqmVN1bVDwo9"
      },
      "id": "tqmVN1bVDwo9"
    },
    {
      "cell_type": "code",
      "source": [
        "def runregression(regressor,dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers=True):\n",
        "  \"\"\" run  all regression models\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "            regressor :select regression models include ()\n",
        "\n",
        "            dataframe  (:class:`DataFrame):main dataframe\n",
        "\n",
        "            input_columns(list)  : name of input columns \n",
        "\n",
        "            dependent_var(str) : dependent varvariable column  \n",
        "            \n",
        "            train(int) : train splite  \n",
        "            \n",
        "            test(int) :   test splite \n",
        "\n",
        "            treat_outliers(bool) : treating outliers\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            result (:class:`DataFrame) : return all acaccuracy  regression models\n",
        "              \n",
        "            \"\"\"\n",
        "  def FindMtype(regressor):\n",
        "        # Intstantiate Model\n",
        "        M = regressor\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "  Mtype = FindMtype(regressor)    \n",
        "  #set up your results table\n",
        "  columns = ['Regressor', 'Result']\n",
        "  vals = [(\"Place Holder\",\"N/A\")]\n",
        "  results = spark.createDataFrame(vals, columns)\n",
        "  if Mtype == \"LinearRegression\":\n",
        "    new_result = LinearRegressionmodel(dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers)\n",
        "    results = results.union(new_result)\n",
        "    results = results.where(\"Regressor!='Place Holder'\")\n",
        "  elif Mtype == \"RandomForestRegressor\":\n",
        "    new_result = RandomForestRegressormodel(dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers)\n",
        "    results = results.union(new_result)\n",
        "    results = results.where(\"Regressor!='Place Holder'\")\n",
        "\n",
        "  elif Mtype in(\"GBTRegressor\"):\n",
        "    new_result = GBTRegressormodel(dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers)\n",
        "    results = results.union(new_result)\n",
        "    results = results.where(\"Regressor!='Place Holder'\")\n",
        "  elif Mtype in(\"DecisionTreeRegressor\"):  \n",
        "    new_result = DecisionTreeRegressormodel(dataframe,input_columns,dependent_var,trainesplite,testsplite,treat_outliers)\n",
        "    results = results.union(new_result)\n",
        "    results = results.where(\"Regressor!='Place Holder'\")\n",
        "  return results"
      ],
      "metadata": {
        "id": "OozGOuOI0ED4"
      },
      "id": "OozGOuOI0ED4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The best regressor model has the lowest root mean squared error (rmse)"
      ],
      "metadata": {
        "id": "sn4xAq8jPy0m"
      },
      "id": "sn4xAq8jPy0m"
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(bestmodel,dataframe,input_columns:list,dependent_var:str,treat_outliers=True)->tuple:\n",
        "\n",
        "  \"\"\" return tuple with predictions and percentage of diffrence between actual data and predition \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "            bestmodel :select regression models include ()\n",
        "\n",
        "            dataframe  (:class:`DataFrame):main dataframe\n",
        "\n",
        "            input_columns(list)  : name of input columns \n",
        "\n",
        "            dependent_var(str) : dependent varvariable column  \n",
        "\n",
        "\n",
        "            treat_outliers(bool) : treating outliers\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            perdiction_df (tuple) : tuple with two dataframes: (predictions) and (percentage of diffrence between actual data and predition )\n",
        "              \n",
        "            \"\"\"\n",
        "  def FindMtype(bestmodel):\n",
        "        # Intstantiate Model\n",
        "        M = bestmodel\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        return Mtype\n",
        "    \n",
        "  Mtype = FindMtype(bestmodel)\n",
        "  final_data=MLRegressDFPrep(dataframe,input_columns,dependent_var,treat_outliers)  \n",
        "  test = final_data.limit(10)\n",
        "  if Mtype == \"LinearRegression\":\n",
        "      predictions = LR_BestModel.transform(test) \n",
        "  if Mtype in(\"DecisionTreeRegressor\"):\n",
        "      predictions = DT_BestModel.transform(test)\n",
        "  if Mtype in(\"GBTRegressor\"):\n",
        "      predictions = GBT_BestModel.transform(test)\n",
        "  if Mtype in(\"RandomForestRegressor\"):\n",
        "    predictions = RF_BestModel.transform(test)\n",
        "\n",
        "  predictions = predictions.withColumn(\"difference\",predictions.prediction-predictions.label) \\\n",
        "                          .withColumn(\"diff perct\",((predictions.prediction-predictions.label)/predictions.label)*100)\n",
        "  diff_perct=predictions.describe(['diff perct'])\n",
        "  perdiction_df=predictions,diff_perct\n",
        "  return perdiction_df\n",
        " "
      ],
      "metadata": {
        "id": "W_AggedhPrjw"
      },
      "id": "W_AggedhPrjw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def featureimportances(n:int,model)->list:\n",
        "    \"\"\" return feature importances list\n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "\n",
        "            n(int) : number of input value\n",
        "            \n",
        "            model : select model \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            feature importances (list) : return feature importances list\n",
        "              \n",
        "            \"\"\"\n",
        "    def FindMtype(model):\n",
        "        # Intstantiate Model\n",
        "        M = model\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "        \n",
        "        \n",
        "        return Mtype\n",
        "    Mtype = FindMtype(model)\n",
        "    if Mtype == \"LinearRegression\":\n",
        "        featureimportances=LR_Pvalues\n",
        "    if Mtype in(\"DecisionTreeRegressor\"):\n",
        "        featureimportances=DT_featureImportances.argsort()[-n:][::-1]\n",
        "    if Mtype in(\"GBTRegressor\"):\n",
        "        featureimportances=GBT_featureImportances.argsort()[-n:][::-1]\n",
        "    if Mtype in(\"RandomForestRegressor\"):\n",
        "        featureimportances=RF_featureImportances.argsort()[-n:][::-1]\n",
        "   \n",
        "    \n",
        "    return featureimportances"
      ],
      "metadata": {
        "id": "heo_Ek0AwFia"
      },
      "id": "heo_Ek0AwFia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testmodel(bestmodel,values:list,input_columns:list,skewtreat:bool=False,skewtreatcolumns:list=None): \n",
        "  \"\"\" return a predition for actual list of input \n",
        "             \n",
        "        Parameters:\n",
        "        -------------- \n",
        "            bestmodel :select regression models include ()\n",
        "\n",
        "            values(list) : list of input number\n",
        "\n",
        "\n",
        "            input_columns(list)  : name of input columns \n",
        "\n",
        "\n",
        "            skewtreat(bool) : treat for skew columns  \n",
        "\n",
        "\n",
        "            skewtreatcolumns(list) : list of column need fo skew\n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            perdictions (:class:`DataFrame) : return prediction dataframe \n",
        "              \n",
        "            \"\"\"\n",
        " \n",
        "  def FindMtype(bestmodel):\n",
        "        # Intstantiate Model\n",
        "        M = bestmodel\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__  \n",
        "        return Mtype\n",
        "  Mtype = FindMtype(bestmodel)\n",
        "  column_names = input_columns\n",
        "  test = spark.createDataFrame(values,column_names)\n",
        "  if skewtreat:\n",
        "      for skewcol in skewtreatcolumns:   \n",
        "        test = test.withColumn(skewcol, log(skewcol) +1)\n",
        "\n",
        "  # Transform for a vector\n",
        "  features_list = input_columns\n",
        "  assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
        "  test = assembler.transform(test).select('features')\n",
        "      \n",
        "  # rescale each feature to range [min, max].\n",
        "  scaled_test = scalerModel.transform(test)\n",
        "  final_test = scaled_test.select('scaledFeatures')\n",
        "  final_test = final_test.withColumnRenamed('scaledFeatures','features')\n",
        "  if Mtype == \"LinearRegression\":\n",
        "      predictions = LR_BestModel.transform(final_test) \n",
        "  if Mtype in(\"DecisionTreeRegressor\"):\n",
        "      predictions = DT_BestModel.transform(final_test)\n",
        "  if Mtype in(\"GBTRegressor\"):\n",
        "      predictions = GBT_BestModel.transform(final_test)\n",
        "  if Mtype in(\"RandomForestRegressor\"):\n",
        "    predictions = RF_BestModel.transform(final_test)\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "aONt4ViY6CLK"
      },
      "id": "aONt4ViY6CLK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequent Pattern Mining in PySpark's MLlib"
      ],
      "metadata": {
        "id": "8cBJALoHeaxo"
      },
      "id": "8cBJALoHeaxo"
    },
    {
      "cell_type": "code",
      "source": [
        "def FPM(itemsCol:str,p_types,minSupport:int,minConfidence:int,maxPatternLength:int,df_array)->tuple:\n",
        "  \"\"\" return  itempopularity,assoc,predict,sequence dataframes\n",
        "             \n",
        "        Parameters:\n",
        "        --------------\n",
        "            \n",
        "            itemsCol(str) : p_types alias column\n",
        "                \n",
        "            p_types(:class:`DataFrame) : DataFrame with  unique values in each row\n",
        "\n",
        "            minSupport(int) : minimum of support        \n",
        "            \n",
        "            minConfidence(int) : minimum of Confidence \n",
        "\n",
        "            maxPatternLength(int) : maximum Pattern Length\n",
        "            \n",
        "            df_array(:class:`DataFrame) : DataFrame with  array of array\n",
        "\n",
        " \n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            silhcent (list) : return a tuple with silhouette,center of kmeans ,prediction dataframe \n",
        "\n",
        "        Example:\n",
        "        --------------  \n",
        "\n",
        "            p_types = df.withColumn(\"vert\",expr(\"CASE WHEN EXT1 in('4','5') or EXT5 in('4','5') or EXT7 in('4','5') or EXT9 in('4','5') THEN 'extrovert' WHEN EXT1 in('1','2') or EXT5 in('1','2') or EXT7 in('1','2') or EXT9 in('1','2') THEN 'introvert' ELSE 'neutrovert' END AS vert\"))\n",
        "            p_types = p_types.withColumn(\"mood\",expr(\"CASE WHEN EST2 in('4','5') THEN 'chill' WHEN EST2 in('1','2') THEN 'highstrung' ELSE 'neutral' END AS mood\"))\n",
        "\n",
        "            p_types = p_types.select(array('mood', 'vert').alias(\"items\"))\n",
        "            p_types.limit(4).toPandas()\n",
        "\n",
        "            df_array = df.select(array(array('EXT1', 'EXT2'),array('EXT3','EXT4'),array('EXT5','EXT6'),array('EXT7','EXT8'),array('EXT9','EXT10')).alias(\"sequence\"))\n",
        "\n",
        "           \"\"\" \n",
        "  from pyspark.ml.fpm import FPGrowth\n",
        "  from pyspark.ml.fpm import PrefixSpan\n",
        "  fpGrowth = FPGrowth(itemsCol=itemsCol, minSupport=minSupport, minConfidence=minConfidence)\n",
        "  model = fpGrowth.fit(p_types)\n",
        "  itempopularity = model.freqItemsets\n",
        "\n",
        "  assoc = model.associationRules\n",
        "\n",
        "\n",
        "  predict = model.transform(p_types)\n",
        "\n",
        "\n",
        "  prefixSpan = PrefixSpan(minSupport=minSupport, maxPatternLength=maxPatternLength)\n",
        "\n",
        "  sequence = prefixSpan.findFrequentSequentialPatterns(df_array)\n",
        "  rolebase=itempopularity,assoc,predict,sequence\n",
        "  return rolebase\n"
      ],
      "metadata": {
        "id": "vCKkI_WGehF9"
      },
      "id": "vCKkI_WGehF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence(itemsCol:str,p_types,minSupport:int,minConfidence:int,maxPatternLength:int,df_array):\n",
        "  \"\"\" return  itempopularity,assoc,predict,sequence dataframes\n",
        "             \n",
        "        Parameters:\n",
        "        --------------\n",
        "            \n",
        "            itemsCol(str) : p_types alias column\n",
        "                \n",
        "            p_types(:class:`DataFrame) : DataFrame with  unique values in each row\n",
        "\n",
        "            minSupport(int) : minimum of support        \n",
        "            \n",
        "            minConfidence(int) : minimum of Confidence \n",
        "\n",
        "            maxPatternLength(int) : maximum Pattern Length\n",
        "            \n",
        "            df_array(:class:`DataFrame) : DataFrame with  array of array\n",
        "\n",
        " \n",
        "            \n",
        "            \n",
        "                       \n",
        "        Returns:\n",
        "        --------------\n",
        "            filtered(:class:`DataFrame) : DataFrame with  sequence percentage \n",
        "\n",
        "        Example:\n",
        "        --------------  \n",
        "\n",
        "            p_types = df.withColumn(\"vert\",expr(\"CASE WHEN EXT1 in('4','5') or EXT5 in('4','5') or EXT7 in('4','5') or EXT9 in('4','5') THEN 'extrovert' WHEN EXT1 in('1','2') or EXT5 in('1','2') or EXT7 in('1','2') or EXT9 in('1','2') THEN 'introvert' ELSE 'neutrovert' END AS vert\"))\n",
        "            p_types = p_types.withColumn(\"mood\",expr(\"CASE WHEN EST2 in('4','5') THEN 'chill' WHEN EST2 in('1','2') THEN 'highstrung' ELSE 'neutral' END AS mood\"))\n",
        "\n",
        "            p_types = p_types.select(array('mood', 'vert').alias(\"items\"))\n",
        "            p_types.limit(4).toPandas()\n",
        "\n",
        "            df_array = df.select(array(array('EXT1', 'EXT2'),array('EXT3','EXT4'),array('EXT5','EXT6'),array('EXT7','EXT8'),array('EXT9','EXT10')).alias(\"sequence\"))\n",
        "\n",
        "           \"\"\" \n",
        "  from pyspark.sql.functions import expr, round\n",
        "  sequence=FPM(itemsCol,p_types,minSupport,minConfidence,maxPatternLength,df_array)\n",
        "\n",
        "  #get the size of each array within the arrays\n",
        "  filtered = sequence.withColumn('size', expr('transform(sequence, x -> size(x))'))\n",
        "  # sequence.withColumn(\"length\",size(col(\"sequence\")))\n",
        "\n",
        "  # Let's also add a column that tells us the percentage of the sequences\n",
        "  row_cnt = df_array.count()\n",
        "  filtered = filtered.withColumn('percentage',round((col(\"freq\")/row_cnt)*100,2))\n",
        "  # Then filter out only the ones with more than 2 elements\n",
        "  filtered = filtered.where(array_contains(filtered.size, 2))\n",
        "\n",
        "  return filtered\n"
      ],
      "metadata": {
        "id": "Jvkl0dg0ekhx"
      },
      "id": "Jvkl0dg0ekhx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pyspark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "9892cabe730e3a74d4af14052771a14d61c523133c5de90a0ad12cc0d58af27d"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('dic')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}